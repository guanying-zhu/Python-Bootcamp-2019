{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# List Comprehension\n",
    "\n",
    "Today we will analyze some real world data in Python. Before doing that, let's introduce another basic tool in Python: list comprehension.\n",
    "\n",
    "This is a compact way to generate lists that avoids writing a bunch of `for` loops. \n",
    "\n",
    "Let's look at an example. Suppose I want to create a list containing the squares of elements in a given list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "L = [1,2,3,4]\n",
    "\n",
    "# One option\n",
    "squares_forLoop = []\n",
    "for j in L:\n",
    "    squares_forLoop.append(j*j)\n",
    "    \n",
    "# List comprehension version\n",
    "squares_listComprehension = [j*j for j in L]\n",
    "\n",
    "print(squares_forLoop)\n",
    "print(squares_listComprehension)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The \"list comprehension\" version is equivalent to the `for` loop version. The only difference is cleaner looking code. \n",
    "\n",
    "Conditions in the list comphrension code can be more complex. Suppose that we want to take a sentence and pull out all digits which appear in the sentence. We will use the built-in function `x.isdigit()` which returns `True` if `x` is a digit and `False` otherwise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sentence = 'This is a 1234 sentence 567'\n",
    "\n",
    "# List comprehension version\n",
    "digits_LC = [x for x in sentence if x.isdigit()]\n",
    "\n",
    "# For loop version\n",
    "digits_FL = []\n",
    "for x in sentence:\n",
    "    if x.isdigit():\n",
    "        digits_FL.append(x)\n",
    "\n",
    "\n",
    "print(digits_LC)\n",
    "print(digits_FL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "List comprehensions can also contain multiple `for` loops. Let's create a flattened version of  a list of lists: i.e. a single list containing all entries of the input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = [[1,2],[3,4]]\n",
    "\n",
    "flattened = [x for row in A for x in row]\n",
    "flattened"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise\n",
    "\n",
    "For the lists `L` and `M` below, create a new list `N` containing elements of `L` which do not appear in `M`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input lists\n",
    "L = list(range(25))\n",
    "M = [1,4,20,22,35]\n",
    "\n",
    "# Create N below\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### List Comprehension vs. Map\n",
    "\n",
    "The examples that we did above could have been accomplished using `map` and `lambda` (or just a plain `for` loop). The method that you choose is more-or-less a judgment call. \n",
    "\n",
    "Remember an overarching goal: produce efficient, but readable code! \n",
    "\n",
    "The `map` function can do some things that list comprehension cannot (see the 'lazy evaluation' example from last time). It can also give small speed advantages if you are mapping a function which is already defined. On the other hand, for a very complicated list, it might make more sense to just use a `for` loop for the sake of readability\n",
    "\n",
    "In general, I think that most people prefer to use list comphrehension where possible."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reading and Cleaning Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Last time we studied statistics on some data that we generated. Now let's try to work with some real world data. There are many powerful Python packages for data wrangling, analysis and visualization. To start, we will just work in basic Python and write our own functions as needed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start by writing a function to read a file into Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_file(file_name):\n",
    "    file = open(file_name) # Gets 'file_name' as a file object\n",
    "    data = file.read() # Reads the file object into a string object\n",
    "    file.close() # Closes the file object, since we don't need it any more\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we use our function to read the 'global_temperature_anomaly.csv' file into Python. This file is saved in the 'data' folder, so we need to tell Python where to find it. The '.csv' extension means that the data is stored as 'comma separated values'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_as_string = read_file('data/global_temperature_anomaly.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the length of the string\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can view a part of the data by looking at a slice\n",
    "data_as_string[:600]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This looks like a mess. We can infer that `\\n` is code for 'new line'. This would probably look better if we split the data into a list of lines. There is a built-in function to split a string into a list, given a marker. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_as_lines = data_as_string.split('\\n')\n",
    "# file_name.split(marker) turns the string into a list, \n",
    "# where marker tells us when to start a new entry in the list\n",
    "\n",
    "data_as_lines[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This already looks better! The data records the discrepancy between the average temperature over a given year and the average temperature over the time period 1901-2000 (called  temperature anomalies).\n",
    "\n",
    "Typically, raw data files come with a bunch of 'metadata' giving some description of the data. This is necessary for context, but not useful for the actual data analysis. We shoud 'clean' the data by removing the metadata at the top. \n",
    "\n",
    "The structure of this particular data file is simple. If we look at the full data set, we see that it is already pretty clean after the header. Then we can just count how many lines the metadata takes up and remove them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_as_lines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks like trimming the first 7 lines off would almost do the trick. If the dataset was messier (or if there was just more metadata), then we wouldn't want to eyeball it. For the sake of flexibility, let's write a function to help us explore data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function prints a list with a line number in front of it.\n",
    "def print_list(list):\n",
    "    i = 0\n",
    "    for item in list:\n",
    "        print(i, item)\n",
    "        i = i + 1\n",
    "    return\n",
    "\n",
    "# Let's run it on our data right away\n",
    "print_list(data_as_lines)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Indeed, we want to trim off the first 7 lines. There is also a blank line at the end that we want to get rid of."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "good_lines = data_as_lines[7:-1]  \n",
    "# The '-1' says we slice up to the second-to-last entry\n",
    "\n",
    "good_lines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our data is starting to look pretty clean. Now each entry in the list is of the form `'year,temperature_anomaly'`. It would be more useful to split each of these into its own list `[year, temperature_anomaly]`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [x.split(',') for x in good_lines]\n",
    "\n",
    "# Let's look at a slice of the clean data.\n",
    "data[:4]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise\n",
    "\n",
    "Split the data into two lists: `years` is a list of all of the years and `anomalies` is a list of all anomalies. This can be done using `for` loops, `lambda` functions or list comprhension. Try to do it using list comphrehension!\n",
    "\n",
    "<b>Important: </b> The data is currently stored as strings. We want to store all numbers in both lists as floating point numbers in order to analyze it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have completely clean data, we can visualize it. Let's plot the temperature anomalies against the years."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "\n",
    "plt.plot(years, anomalies)\n",
    "plt.title(\"Temperature Anomalies\")\n",
    "plt.ylabel(\"Degrees C\")\n",
    "plt.xlabel(\"Year\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trends in the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see a clear upward trend in the anomaly data. We now explore some techniques to visualize these trends more effectively. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Smoothing the Data\n",
    "\n",
    "The data is rather noisy: there is lots of short-term variation which may obscure the long-term trend. One approach to removing some of the short-term variation from the picture is to 'smooth' the data.\n",
    "\n",
    "Now we smooth the data using a 'sliding window' approach. Think of the data as a list of ordered pairs $(x,f(x))$ with $x$ a year and $f(x)$ the temperature anomaly for that year. Pick a window length $W$. We construct a new function $F$ defined for each point $x$ in the domain by setting $F(x)$ to be the average value of $f$ over the set $\\{x,x+1,\\ldots,x+W\\}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def smooth(data, window):\n",
    "    output = [] # Initialize with an empty list. We will fill in the values with a for loop.\n",
    "    n = len(data)\n",
    "    for k in range(n - window + 1): \n",
    "        # For values of k which are too large, the data won't be defined on the whole window!\n",
    "        segment = data[k:(k + window)] # Pick out the slice of the data in the window.\n",
    "        value = sum(segment)/window # Average value over the slice\n",
    "        output.append(value)\n",
    "    return output\n",
    "\n",
    "# Note: I used a for loop in this function because (to me) it is more readable than \n",
    "# list comprehension and it allowed me to add comments.\n",
    "\n",
    "# The following function returns the original data, restricted to the domain of the smooth data.\n",
    "def drop_window(data, window):\n",
    "    return data[:-(window-1)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's plot the data and the smoothed data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "window = 10 # Play with this parameter to see the effect of changing the window size\n",
    "years2 = drop_window(years, window)\n",
    "anomalies1 = drop_window(anomalies, window)\n",
    "anomalies2 = smooth(anomalies,window)\n",
    "\n",
    "print (len(years2), len(anomalies1), len(anomalies2))\n",
    "\n",
    "plt.plot( years2, anomalies1, color='red', linestyle='solid')\n",
    "plt.plot( years2, anomalies2, color='blue', linestyle='solid')\n",
    "plt.title(\"Temperature Anomalies\")\n",
    "plt.ylabel(\"Degrees C\")\n",
    "\n",
    "plt.show()\n",
    "\n",
    "# If you want to save your figure, uncomment the next line:\n",
    "# plt.savefig('smoothed_anomalies.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fitting the Data\n",
    "\n",
    "Another common approach to understanding trends in the data would be to approximate it as well as possible by a simpler function. The most obvious class of functions to use would be polynomials. \n",
    "\n",
    "Once again, consider our data as a collection of pairs $(x,f(x))$ (in our example, $x$ is the year and $f(x)$ is the corresponding anomaly), with $x \\in \\{x_1,\\ldots,x_N\\}$. Fix a degree $d$. The goal is to find a degree-$d$ polynomial\n",
    "$$\n",
    "p(x) = c_0 + c_1 x + c_2 x^2 + \\cdots + c_d x^d\n",
    "$$\n",
    "such that $p$ minimizes the total squared error:\n",
    "$$\n",
    "\\sum_{i=1}^N (f(x_i) - p(x_i))^2.\n",
    "$$\n",
    "\n",
    "The numpy package has a built-in function to find the best-fitting polynomial. Let's use it on our anomaly data to find the best-fitting degree-$1$ polynomial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "m, b = np.polyfit(years, anomalies, 1)\n",
    "# The input of polyfit is a list of x values, a list of values f(x), and a degree\n",
    "# The output is the coefficients of the best-fitting polynomial. \n",
    "# Since we are using degree-1, the output is a slope and an intercept.\n",
    "# In general, the output is of the form c_d, c_{d-1}, ... , c_1, c_0 (using the notation above).\n",
    "\n",
    "# The degree-1 polynomial of best fit is given by:\n",
    "p = lambda x:m*x+b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have our best-fitting degree-1 polynomial, we can graph it together with the graph of the true data to visualize how well it really fits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a list by applying p(x) to the years in the data set\n",
    "linfit = [p(x) for x in years]\n",
    "\n",
    "# Draw the data and the line\n",
    "\n",
    "plt.plot(years, anomalies, color='red', linestyle='solid')\n",
    "plt.plot( years, linfit, color='green', linestyle='solid')\n",
    "plt.title(\"Temperature Anomalies\")\n",
    "plt.ylabel(\"Degrees C\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use our linear function to make a prediction about the temperature anomaly in the future. This is called (linear) regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = p(2040)\n",
    "print(\"Predicted Temperature Anomaly in year 2040: \", prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise\n",
    "\n",
    "Does this prediction seem accurate? (The answer is 'no'.)\n",
    "\n",
    "Improve the prediction by using different parameters in polyfit. Justify your claim that your prediction is better by producing a convincing plot.\n",
    "\n",
    "Adjusting the parameters in polyfit too much will 'overfit' the polynomial to the data. Produce a plot to illustrate this phenomenon.\n",
    "\n",
    "You could speed up your experiments by writing a function which finds the best-fitting degree-$d$ function in a single step.\n",
    "\n",
    "Use various prediction models to predict when the temperature anomaly will exceed 2 degrees Celsius."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Homework\n",
    "\n",
    "In the 'data' folder, you will also find data on the sea level since 1993. Using similar methods to what was used here, analyze this sea level data. In particular, try to predict the sea level in 2040. Use the \"smoothed (60-day Gaussian type filter) GMSL (GIA applied) variation (mm); annual and semi-annual signal removed )  with respect to 20-year mean\".\n",
    "\n",
    "This dataset will take a lot more work to explore and clean! You will likely want to use some tricks or functions that we haven't yet covered in class. Google is your friend here (Googling stuff that you don't know how to do is part of the coding experience!). In particular, filter() might be useful here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
