{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Principal Component Analysis\n",
    "\n",
    "Principal Component Analysis is a technique for extracting information about variability in a dataset. It can be used on high-dimensional data as a form of *dimension reduction*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import some useful packages\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.pyplot import figure\n",
    "import seaborn as sns; sns.set()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We begin by creating some toy data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "N = 500 # Number of points to sample\n",
    "m = 0.5 # Slope\n",
    "b = 2   # Intercept\n",
    "x_shift = 4 # Horizontal shift\n",
    "\n",
    "# Sample x values according to a normal distribution, shift them horizonatally and sort them\n",
    "xs = np.random.normal(0,1,N) + x_shift\n",
    "xs = np.sort(xs)\n",
    "\n",
    "# Get y values by applying a linear map to the x values then adding noise\n",
    "ys = m*xs + b + np.random.normal(0,0.5,N)\n",
    "plt.axis('equal')\n",
    "\n",
    "plt.scatter(xs,ys)\n",
    "plt.axis('equal');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have a *point cloud*; this is common terminology for a collection of vectors in some vector space.  Intuitively, there is one direction which explains most of the variation in the data (a vector in the direction of the best fit line). The rest of the variation is in the \"thickness\" of the point cloud, which we can visualize as a shorter vector orthogonal to the first one. The goal of *principal component analysis (PCA)* is to determine these vectors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PCA: Basic Theory\n",
    "\n",
    "Let's make our goal precise. We will organize our data as a \"tall, skinny matrix\" $X \\in \\mathbb{R}^{N \\times d}$, with each of the $N$ ($N=500$ in our example) rows giving a vector in $\\mathbb{R}^d$ ($d = 2$ in our example). This is called the *data matrix*. Denote the rows by $\\vec{x}_1,\\ldots,\\vec{x}_N$, each $\\vec{x}_j \\in \\mathbb{R}^d$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array([xs,ys]).T # Form a \"tall, skinny matrix\" containing the data\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is standard to assume that the columns of $X$ have mean zero. This loses no generality, because we can preprocess by shifting, then shift back at the end if we want to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mu0 = np.mean(X[:,0])\n",
    "mu1 = np.mean(X[:,1])\n",
    "\n",
    "X_centered = X - np.array([mu0,mu1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(X[:,0],X[:,1])\n",
    "plt.scatter(X_centered[:,0],X_centered[:,1])\n",
    "plt.axis('equal');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first goal is to find a unit vector which lines up as closely with as many of the $\\vec{x}_j$'s as it possibly can. That is, we want to find \n",
    "$$\n",
    "\\vec{v}_1 = \\mathrm{argmax}_{\\|\\vec{v}\\| = 1} \\sum_j (\\vec{x}_j \\cdot \\vec{v})^2.\n",
    "$$\n",
    "This defines our first *principal vector*. \n",
    "\n",
    "To get the next principal vector, we look for the direction of greatest variation which is orthogonal to $\\vec{v}_1$. More precisely, the second principal vector $\\vec{v}_2$ is \n",
    "$$\n",
    "\\vec{v}_2 = \\mathrm{argmax} \\left\\{ \\sum_j (\\vec{x}_j \\cdot \\vec{v})^2 : \\|\\vec{v}\\| = 1 \\mbox{ and } \\vec{v} \\cdot \\vec{v}_1 = 0\\right\\}.\n",
    "$$\n",
    "This can be continued inductively to find all principal vectors $\\vec{v}_1,\\vec{v}_2, \\ldots, \\vec{v}_d$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Digging into the math a bit more, note that\n",
    "\\begin{align*}\n",
    "\\max_{\\|\\vec{v}\\| = 1} \\sum_j (\\vec{x}_j \\cdot \\vec{v})^2 &= \\max_{\\|\\vec{v}\\| = 1} \\left\\|X \\vec{v} \\right\\|^2 \\\\\n",
    "&= \\max_{\\|\\vec{v}\\| = 1} \\vec{v}^T X^T X \\vec{v} \\\\\n",
    "&= \\max_{\\vec{v} \\neq \\vec{0}} \\frac{\\vec{v}^T X^T X \\vec{v}}{\\|\\vec{v}\\|^2}.\n",
    "\\end{align*}\n",
    "It is not hard to show that this max value is the maximum eigenvalue of $X^T X$, realized by the corresponding (unit)  eigenvector. Similarly, the remaining singular vectors are obtained as the other eigenvectors of $X^T X$ (arranged in decreasing order of eigenvalue).\n",
    "\n",
    "\n",
    "\n",
    "The matrix $X^T X \\in \\mathbb{R}^{d \\times d}$ is called the *covariance matrix of $X$*.\n",
    "\n",
    "#### Conclusion\n",
    "\n",
    "The singular vectors of the centered data matrix $X$ are the unit eigenvectors (well-defined up to sign $\\pm 1$) of the covariance matrix $X^T X$, listed in descending order of corresponding eigenvalue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "cov = X_centered.T@X_centered\n",
    "cov"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can easily compute the eigenvalues of the covariance matrix using `numpy`, as we do below. The output of `np.linalg.eig` is a \"tuple\" `eVals, eVec`. The variable `eVals` stores the eigenvalues in descending order. The variable `eVec` stores the corresponding eigenvectors as **columns**. The eigenvectors are normalized (i.e., they are unit vectors)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "eVals, eVec = np.linalg.eig(cov)\n",
    "print(eVals)\n",
    "print(eVec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note the the \"slope\" of the first eigenvector is pretty close to the slope that used to construct our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eVec[1,0]/eVec[0,0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can plot the principal vectors over the pointcloud using the `quiver` function. The principal vectors are scaled by their respective eigenvalues to illustrate the difference in variability quantification. \n",
    "\n",
    "The `scale` option has been tuned to give a good picture (higher values scale the vectors down more --- note that the norms of the true principal vectors are quite large!). The same scale is used for both vectors to illustrate their relative lengths."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.scatter(X_centered[:, 0], X_centered[:, 1], alpha=0.2)\n",
    "# alpha controls 'opacity' of the points\n",
    "\n",
    "# Syntax for quiver:\n",
    "# plt.quiver(xVal for basepoint, yVal for basepoint, xVal for vector, yVal for vector, scale = )\n",
    "plt.quiver(0, 0, eVals[0]*eVec[0,0], eVals[0]*eVec[1,0], scale=1200)\n",
    "plt.quiver(0, 0, eVals[1]*eVec[0,1], eVals[1]*eVec[1,1], scale=1200)\n",
    "\n",
    "plt.axis('equal');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We could also shift everything back to the original data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(X[:, 0], X[:, 1], alpha=0.2)\n",
    "\n",
    "# Syntax for quiver:\n",
    "# plt.quiver(xVal for basepoint, yVal for basepoint, xVal for vector, yVal for vector, scale = )\n",
    "plt.quiver(mu0, mu1, eVals[0]*eVec[0,0], eVals[0]*eVec[1,0], scale=1200)\n",
    "plt.quiver(mu0, mu1, eVals[1]*eVec[0,1], eVals[1]*eVec[1,1], scale=1200)\n",
    "\n",
    "plt.axis('equal');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PCA with SVD\n",
    "\n",
    "The *Singular Value Decomposition (SVD)* expresses $X$ as \n",
    "$$\n",
    "X = U \\Sigma W^T,\n",
    "$$\n",
    "where $U \\in \\mathbb{R}^{N \\times N}$ and $W \\in \\mathbb{R}^{d \\times d}$ are matrices with orthonormal column vectors (orthogonal matrices) and $\\Sigma \\in \\mathbb{R}^{N \\times d}$ is a diagonal matrix with the square roots of the eigenvalues of $X^T X$ on its diagonal. These are called the *singular values* of $X$.\n",
    "\n",
    "We calculate the SVD of our centered data using the code below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "U, Sigma, Wt = np.linalg.svd(X_centered)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(U.shape)\n",
    "print(Sigma.shape)\n",
    "print(Wt.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the shapes, we see that `Sigma` stores only the nonzero entries of $\\Sigma$ (not the full matrix in $\\mathbb{R}^{N \\times d}$). These are given in descending order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Sigma"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we square these values, we should get the eigenvalues of $X^T X$ that we computed earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(Sigma**2) # Note that this syntax applies the **2 operation to each entry in the array\n",
    "print(eVals)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And if we examine the matrix $W^T$, we see that its columns are (up to a sign) the same as the eigenvectors of $X^T X$ we computed earlier!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(Wt)\n",
    "print(eVec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Indeed, this must be the case in general, since\n",
    "\\begin{align*}\n",
    "X^T X &= (U\\Sigma W^T)^T (U \\Sigma W^T) \\\\\n",
    "&= W \\Sigma^T U^T U \\Sigma W^T \\\\\n",
    "&= W \\Sigma^T \\Sigma W^T \\\\\n",
    "&= W \\widehat{\\Sigma} W^T,\n",
    "\\end{align*}\n",
    "where $\\widehat{\\Sigma}$ is a diagonal matrix containing the eigenvalues of $X^T X$. It follows that the columns of $W^T$ are the eigenvectors of $X^T X$. \n",
    "\n",
    "#### Conclusion\n",
    "\n",
    "The principal vectors of $X$ are given by the columns of $W^T$ from the SVD of $X$ (read from left-to-right).\n",
    "\n",
    "### Theoretical Homework\n",
    "\n",
    "Verify the steps and statements made in the above proof."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise\n",
    "\n",
    "Write a function to compute the principal vectors of a 2-dimensional point cloud. Use it to perform PCA on the examples `X1`, `X2` and `X3` defined below. Plot the results.\n",
    "\n",
    "For plotting purposes, it may be useful to have your function return a 'tuple'; e.g.\n",
    "`return X_centered, Sigma, Wt`\n",
    "or something along those lines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Example 1 generation\n",
    "mean1 = [0, 0]\n",
    "cov1 = [[10, 0], [0, 10]]\n",
    "\n",
    "X1 = np.random.multivariate_normal(mean1, cov1, 500)\n",
    "\n",
    "# Example 2 generation\n",
    "mean2 = [2, -3]\n",
    "cov2 = [[1, 1], [1, 10]]\n",
    "\n",
    "X2 = np.random.multivariate_normal(mean2, cov2, 500)\n",
    "\n",
    "# Example 3 generation\n",
    "xs = np.random.uniform(0,2*np.pi,500)\n",
    "xs = np.sort(xs)\n",
    "ys = 2*np.sin(xs) + np.random.normal(0,0.5,500)\n",
    "X3 = np.array([xs,ys]).T\n",
    "\n",
    "fig = plt.figure(figsize=(10,5))\n",
    "\n",
    "fig.add_subplot(1,3,1)\n",
    "plt.scatter(X1[:,0],X1[:,1])\n",
    "plt.axis('equal')\n",
    "\n",
    "fig.add_subplot(1,3,2)\n",
    "plt.scatter(X2[:,0],X2[:,1])\n",
    "plt.axis('equal')\n",
    "\n",
    "fig.add_subplot(1,3,3)\n",
    "plt.scatter(X3[:,0],X3[:,1])\n",
    "plt.axis('equal')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PCA with SciKit-Learn\n",
    "\n",
    "Now that we have a solid theoretical understanding of PCA, we can use a built in function from `scikit-learn` to do the computation. This will work in arbitrary dimension, unlike the function we created above.\n",
    "\n",
    "Let's try it on the dataset `X` we have been using to make sure it agrees."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(n_components=2) # Specify the number of principal directions you want to find\n",
    "pca.fit(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check the principal vectors with `pca.components_` and compare to our earlier result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(pca.components_)\n",
    "print(eVec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Careful! We should notice that the principal vectors we are after are given by the *rows* of `pca.components_`. Previously we had been using *columns* of matrices. Once we notice that difference, everything agrees up to a sign.\n",
    "\n",
    "The `pca.singular_values_` method pulls the singular values of $X$, or the square roots of the eigenvalues of $X^T X$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca.singular_values_**2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perhaps a more useful way to compare how variance is captured by each principal vector is to look ath the *explained variance ratio*. If $\\lambda_1,\\ldots,\\lambda_d$ are the eigenvalues of $X^T X$ (listed in descending order), then the explained variance ratios are given by $\\frac{\\lambda_j}{\\sum_k \\lambda_k}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca.explained_variance_ratio_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PCA as Dimension Reduction\n",
    "\n",
    "Let's return to our favorite dataset, MNIST."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_digits\n",
    "digits = load_digits()\n",
    "digits.data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can think of the MNIST dataset as a point cloud in $\\mathbb{R}^{8 \\times 8} \\approx \\mathbb{R}^{64}$. This means it is impossible to visualize directly. On the other hand, we've seen that the data is structured enough that simple classifiation algorithms like logistic regression and SVM work extremely well. This leads us to believe that we may be able to get some sort of visualization of the data by exploiting its special structure.\n",
    "\n",
    "We begin by applying PCA to MNIST."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA()\n",
    "pca.fit(digits.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pca.components_.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we look at the explained variance ratios, we observe the following. Even though the data lives in $64$ dimensions, if we use the principal vector basis then any direction after the first 10 contributes less than 2\\% of the total variance in the data!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.plot(pca.explained_variance_ratio_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we only used the first couple of principal vectors as our coordinate axes, we might actually get a reasonable picture of the MNIST dataset!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pVec0 = pca.components_[0]\n",
    "pVec1 = pca.components_[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pVec0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Even though each principal vector lives in $\\mathbb{R}^64$, the first two of them still span a 2-dimensional plane. We can orthogonally project each point from the MNIST dataset onto this 2-dimensional plane. This is accomplished by taking dot products with each of the principal vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "projectedMNIST = []\n",
    "\n",
    "for j in range(1797):\n",
    "    projectedMNIST.append([np.dot(digits.data[j],pVec0),np.dot(digits.data[j],pVec1)])\n",
    "\n",
    "projectedMNIST = np.array(projectedMNIST)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can plot the projected point cloud on this two dimensional plane."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,10))\n",
    "\n",
    "plt.scatter(projectedMNIST[:, 0], projectedMNIST[:, 1],\n",
    "            c=digits.target, alpha=0.5,\n",
    "            cmap=plt.cm.get_cmap('nipy_spectral_r', 10))\n",
    "\n",
    "plt.xlabel('pVec0')\n",
    "plt.ylabel('pVec1')\n",
    "plt.axis('equal')\n",
    "plt.colorbar();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This gives a relatively faithful picture of the 64-dimensional MNIST dataset in only 2-dimensions. We can see the separation in the classes and this makes it more clear why logistic regression and SVM did such a good job of separating the data.\n",
    "\n",
    "The procedure of finding a low-dimensional representation of high-dimensional data is called *dimension reduction*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Of course this dimension reduction procedure is built into `scikit-learn`. The same effect is achieved by the following code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(2)  # project from 64 to 2 dimensions\n",
    "projected = pca.fit_transform(digits.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.pyplot import figure\n",
    "\n",
    "plt.figure(figsize = (10,10))\n",
    "\n",
    "plt.scatter(projected[:, 0], projected[:, 1],\n",
    "            c=digits.target, alpha=0.5,\n",
    "            cmap=plt.cm.get_cmap('nipy_spectral_r', 10))\n",
    "\n",
    "plt.xlabel('component 1')\n",
    "plt.ylabel('component 2')\n",
    "plt.axis('equal')\n",
    "plt.colorbar();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise\n",
    "\n",
    "Create a 3D scatterplot showing the projection of the MNIST data onto the first 3 principal directions. \n",
    "\n",
    "Hints:\n",
    "- Look here for an idea of how to create 3D scatterplots https://matplotlib.org/2.1.1/gallery/mplot3d/scatter3d.html\n",
    "- Use the command `%matplotlib notebook` to get a 3D plot which you can drag to rotate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise\n",
    "\n",
    "Apply similar dimension reduction techniques on the `fashion-mnist` and/or `olivetti_faces` datasets that we have studied previously. Are these datasets similarly well-separated?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise\n",
    "\n",
    "Create \"random digits\" as follows. Create a data matrix containing only the vectors for a fixed choice of digit in MNIST. Center the data matrix then compute its covariance matrix. Use this as the covariance matrix in the multivariate normal sampling function (this was used above to create examples). A sampled point is your 'random digit'. You can visualize it by reshaping and using `plt.imshow`.\n",
    "\n",
    "Try classifying your random digits with a classifier that you train on MNIST. How often are they correctly classified?\n",
    "\n",
    "**Remark:** When I tried this my random digits looked pretty bad, although they were still recognizable. Can you find a way to improve this algorithm to produce nicer looking results?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
