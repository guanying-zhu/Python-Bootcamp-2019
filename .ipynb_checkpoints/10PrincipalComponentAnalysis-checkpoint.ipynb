{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# High Dimensional Data\n",
    "\n",
    "Principal Component Analysis (PCA) is a technique for extracting information about variability in a dataset. It can be used on high-dimensional data as a form of *dimension reduction*.\n",
    "\n",
    "High-dimensional data often behaves in a way which doesn't agree with our intuition coming from 1,2 and 3-dimensional geometry. Before beginning our study of PCA, let's explore one of these counterintuitive properties, called the *concentration of measure phenomenon* https://en.wikipedia.org/wiki/Concentration_of_measure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import packages\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Write a function to uniformly sample npoints on a unit sphere sitting in R^{dim}\n",
    "def sample_spherical(npoints, dim):\n",
    "    vec = np.random.randn(dim, npoints)\n",
    "    vec /= np.linalg.norm(vec, axis=0)\n",
    "    vec = np.transpose(vec)\n",
    "    return vec\n",
    "\n",
    "# Spherical distance is given by the following function\n",
    "def spherical_dist(p1,p2):\n",
    "    if np.linalg.norm(p1-p2) < 1e-10:\n",
    "        d = 0\n",
    "    else:\n",
    "        d = np.arccos(np.dot(p1,p2))\n",
    "    return d\n",
    "\n",
    "# Given a bunch of samples S on a sphere, we compute the list of all pairwise distances\n",
    "# We don't care about distances between a point and itself\n",
    "# We are also careful to not 'double count', since distance is symmetric.\n",
    "def distance_list(S):\n",
    "    N = S.shape[0]\n",
    "    dist = [spherical_dist(S[0],S[j]) for j in range(1,N)]\n",
    "    for j in range(1,N):\n",
    "        dist = dist + [spherical_dist(S[j],S[k]) for k in range(j+1,N)]\n",
    "    return dist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise\n",
    "\n",
    "Pick an ambient dimension `dim`, sample 100 points on the unit sphere of that dimension, then plot a histogram of the pairwise distances, with the x-axis always running over all possible pairwise distances $[0,\\pi]$. \n",
    "\n",
    "Try this for several 'low' dimensions, then several 'high' dimensions. What do you notice?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dim =  # Pick a dimension here\n",
    "\n",
    "S = sample_spherical(100,dim)\n",
    "\n",
    "dist = distance_list(S)\n",
    "\n",
    "plt.hist(dist, bins=100, range=[0, np.pi], align='mid')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Principal Component Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We begin by creating some toy data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "N = 500 # Number of points to sample\n",
    "m = 0.5 # Slope\n",
    "b = 2   # Intercept\n",
    "x_shift = 4 # Horizontal shift\n",
    "\n",
    "# Sample x values according to a normal distribution, shift them horizonatally and sort them\n",
    "xs = np.random.normal(0,1,N) + x_shift\n",
    "xs = np.sort(xs)\n",
    "\n",
    "# Get y values by applying a linear map to the x values then adding noise\n",
    "ys = m*xs + b + np.random.normal(0,0.5,N)\n",
    "plt.axis('equal')\n",
    "\n",
    "plt.scatter(xs,ys)\n",
    "plt.axis('equal');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have a *point cloud*; this is common terminology for a collection of vectors in some vector space.  Intuitively, there is one direction which explains most of the variation in the data (a vector in the direction of the best fit line). The rest of the variation is in the \"thickness\" of the point cloud, which we can visualize as a shorter vector orthogonal to the first one. The goal of *principal component analysis (PCA)* is to determine these vectors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PCA: Basic Theory\n",
    "\n",
    "Let's make our goal precise. We will organize our data as a \"tall, skinny matrix\" $X \\in \\mathbb{R}^{N \\times d}$, with each of the $N$ ($N=500$ in our example) rows giving a vector in $\\mathbb{R}^d$ ($d = 2$ in our example). This is called the *data matrix*. Denote the rows by $\\vec{x}_1,\\ldots,\\vec{x}_N$, each $\\vec{x}_j \\in \\mathbb{R}^d$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array([xs,ys]).T # Form a \"tall, skinny matrix\" containing the data\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is standard to assume that the columns of $X$ have mean zero. This loses no generality, because we can preprocess by shifting, then shift back at the end if we want to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mu0 = np.mean(X[:,0])\n",
    "mu1 = np.mean(X[:,1])\n",
    "\n",
    "X_centered = X - np.array([mu0,mu1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(X[:,0],X[:,1])\n",
    "plt.scatter(X_centered[:,0],X_centered[:,1])\n",
    "plt.axis('equal');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first goal is to find a unit vector which lines up as closely with as many of the $\\vec{x}_j$'s as it possibly can. That is, we want to find \n",
    "$$\n",
    "\\vec{v}_1 = \\mathrm{argmax}_{\\|\\vec{v}\\| = 1} \\sum_j (\\vec{x}_j \\cdot \\vec{v})^2.\n",
    "$$\n",
    "This defines our first *principal vector*. \n",
    "\n",
    "To get the next principal vector, we look for the direction of greatest variation which is orthogonal to $\\vec{v}_1$. More precisely, the second principal vector $\\vec{v}_2$ is \n",
    "$$\n",
    "\\vec{v}_2 = \\mathrm{argmax} \\left\\{ \\sum_j (\\vec{x}_j \\cdot \\vec{v})^2 : \\|\\vec{v}\\| = 1 \\mbox{ and } \\vec{v} \\cdot \\vec{v}_1 = 0\\right\\}.\n",
    "$$\n",
    "This can be continued inductively to find all principal vectors $\\vec{v}_1,\\vec{v}_2, \\ldots, \\vec{v}_d$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Digging into the math a bit more, note that\n",
    "\\begin{align*}\n",
    "\\max_{\\|\\vec{v}\\| = 1} \\sum_j (\\vec{x}_j \\cdot \\vec{v})^2 &= \\max_{\\|\\vec{v}\\| = 1} \\left\\|X \\vec{v} \\right\\|^2 \\\\\n",
    "&= \\max_{\\|\\vec{v}\\| = 1} \\vec{v}^T X^T X \\vec{v} \\\\\n",
    "&= \\max_{\\vec{v} \\neq \\vec{0}} \\frac{\\vec{v}^T X^T X \\vec{v}}{\\|\\vec{v}\\|^2}.\n",
    "\\end{align*}\n",
    "It is not hard to show that this max value is the maximum eigenvalue of $X^T X$, realized by the corresponding (unit)  eigenvector. Similarly, the remaining singular vectors are obtained as the other eigenvectors of $X^T X$ (arranged in decreasing order of eigenvalue).\n",
    "\n",
    "\n",
    "\n",
    "The matrix $X^T X \\in \\mathbb{R}^{d \\times d}$ is called the *covariance matrix of $X$*.\n",
    "\n",
    "#### Conclusion\n",
    "\n",
    "The singular vectors of the centered data matrix $X$ are the unit eigenvectors (well-defined up to sign $\\pm 1$) of the covariance matrix $X^T X$, listed in descending order of corresponding eigenvalue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "cov = X_centered.T@X_centered\n",
    "cov"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can easily compute the eigenvalues of the covariance matrix using `numpy`, as we do below. The output of `np.linalg.eig` is a \"tuple\" `eVals, eVec`. The variable `eVals` stores the eigenvalues in descending order. The variable `eVec` stores the corresponding eigenvectors as **columns**. The eigenvectors are normalized (i.e., they are unit vectors)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "eVals, eVec = np.linalg.eig(cov)\n",
    "print(eVals)\n",
    "print(eVec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note the the \"slope\" of the first eigenvector is pretty close to the slope that used to construct our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eVec[1,0]/eVec[0,0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can plot the principal vectors over the pointcloud using the `quiver` function. The principal vectors are scaled by their respective eigenvalues to illustrate the difference in variability quantification. \n",
    "\n",
    "The `scale` option has been tuned to give a good picture (higher values scale the vectors down more --- note that the norms of the true principal vectors are quite large!). The same scale is used for both vectors to illustrate their relative lengths."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.scatter(X_centered[:, 0], X_centered[:, 1], alpha=0.2)\n",
    "# alpha controls 'opacity' of the points\n",
    "\n",
    "# Syntax for quiver:\n",
    "# plt.quiver(xVal for basepoint, yVal for basepoint, xVal for vector, yVal for vector, scale = )\n",
    "plt.quiver(0, 0, eVals[0]*eVec[0,0], eVals[0]*eVec[1,0], scale=1200)\n",
    "plt.quiver(0, 0, eVals[1]*eVec[0,1], eVals[1]*eVec[1,1], scale=1200)\n",
    "\n",
    "plt.axis('equal');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We could also shift everything back to the original data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(X[:, 0], X[:, 1], alpha=0.2)\n",
    "\n",
    "# Syntax for quiver:\n",
    "# plt.quiver(xVal for basepoint, yVal for basepoint, xVal for vector, yVal for vector, scale = )\n",
    "plt.quiver(mu0, mu1, eVals[0]*eVec[0,0], eVals[0]*eVec[1,0], scale=1200)\n",
    "plt.quiver(mu0, mu1, eVals[1]*eVec[0,1], eVals[1]*eVec[1,1], scale=1200)\n",
    "\n",
    "plt.axis('equal');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PCA with SVD\n",
    "\n",
    "The *Singular Value Decomposition (SVD)* expresses $X$ as \n",
    "$$\n",
    "X = U \\Sigma W^T,\n",
    "$$\n",
    "where $U \\in \\mathbb{R}^{N \\times N}$ and $W \\in \\mathbb{R}^{d \\times d}$ are matrices with orthonormal column vectors (orthogonal matrices) and $\\Sigma \\in \\mathbb{R}^{N \\times d}$ is a diagonal matrix with the square roots of the eigenvalues of $X^T X$ on its diagonal. These are called the *singular values* of $X$.\n",
    "\n",
    "We calculate the SVD of our centered data using the code below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "U, Sigma, Wt = np.linalg.svd(X_centered)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print(U.shape)\n",
    "print(Sigma.shape)\n",
    "print(Wt.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `Sigma` output is a list of singular values. If we actually want the matrix which appears in the decomposition, we can use the `fill_diagonal` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Sigma_diag = np.zeros((500,2))\n",
    "np.fill_diagonal(Sigma_diag,Sigma) # Note this function changes the value of Sigma_diag\n",
    "Sigma_diag"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can check that the really decomposition does what we claimed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Take the matrix norm of the difference between X and its SVD\n",
    "np.linalg.norm(X_centered - U@Sigma_diag@Wt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For most applications, just the list of singular values will be useful. Note that the output of the SVD function gives Sigma as a list of singular values in *descending order* of magnitude."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Sigma"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we square these values, we should get the eigenvalues of $X^T X$ that we computed earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(Sigma**2) # Note that this syntax applies the **2 operation to each entry in the array\n",
    "print(eVals)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And if we examine the matrix $W^T$, we see that its columns are (up to a sign) the same as the eigenvectors of $X^T X$ we computed earlier!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(Wt)\n",
    "print(eVec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Indeed, this must be the case in general, since\n",
    "\\begin{align*}\n",
    "X^T X &= (U\\Sigma W^T)^T (U \\Sigma W^T) \\\\\n",
    "&= W \\Sigma^T U^T U \\Sigma W^T \\\\\n",
    "&= W \\Sigma^T \\Sigma W^T \\\\\n",
    "&= W \\widehat{\\Sigma} W^T,\n",
    "\\end{align*}\n",
    "where $\\widehat{\\Sigma}$ is a diagonal matrix containing the eigenvalues of $X^T X$. It follows that the columns of $W^T$ are the eigenvectors of $X^T X$. \n",
    "\n",
    "#### Conclusion\n",
    "\n",
    "The principal vectors of $X$ are given by the columns of $W^T$ from the SVD of $X$ (read from left-to-right).\n",
    "\n",
    "### Theoretical Homework\n",
    "\n",
    "Verify the steps and statements made in the above proof."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise\n",
    "\n",
    "Write a function to compute the principal vectors of a 2-dimensional point cloud. Use it to perform PCA on the examples `X1`, `X2` and `X3` defined below. Plot the results.\n",
    "\n",
    "For plotting purposes, it may be useful to have your function return a 'tuple'; e.g.\n",
    "`return X_centered, Sigma, Wt`\n",
    "or something along those lines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Example 1 generation\n",
    "mean1 = [0, 0]\n",
    "cov1 = [[10, 0], [0, 10]]\n",
    "\n",
    "X1 = np.random.multivariate_normal(mean1, cov1, 500)\n",
    "\n",
    "# Example 2 generation\n",
    "mean2 = [2, -3]\n",
    "cov2 = [[1, 1], [1, 10]]\n",
    "\n",
    "X2 = np.random.multivariate_normal(mean2, cov2, 500)\n",
    "\n",
    "# Example 3 generation\n",
    "xs = np.random.uniform(0,2*np.pi,500)\n",
    "xs = np.sort(xs)\n",
    "ys = 2*np.sin(xs) + np.random.normal(0,0.5,500)\n",
    "X3 = np.array([xs,ys]).T\n",
    "\n",
    "fig = plt.figure(figsize=(10,5))\n",
    "\n",
    "fig.add_subplot(1,3,1)\n",
    "plt.scatter(X1[:,0],X1[:,1])\n",
    "plt.axis('equal')\n",
    "\n",
    "fig.add_subplot(1,3,2)\n",
    "plt.scatter(X2[:,0],X2[:,1])\n",
    "plt.axis('equal')\n",
    "\n",
    "fig.add_subplot(1,3,3)\n",
    "plt.scatter(X3[:,0],X3[:,1])\n",
    "plt.axis('equal')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PCA with SciKit-Learn\n",
    "\n",
    "Now that we have a solid theoretical understanding of PCA, we can use a built in function from `scikit-learn` to do the computation. This will work in arbitrary dimension, unlike the function we created above.\n",
    "\n",
    "Let's try it on the dataset `X` we have been using to make sure it agrees."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(n_components=2) \n",
    "# Specify the number of principal directions you want to find\n",
    "# It defaults to finding all of the them, so it was not necessary to include it here\n",
    "pca.fit(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check the principal vectors with `pca.components_` and compare to our earlier result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(pca.components_)\n",
    "print(eVec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Careful! We should notice that the principal vectors we are after are given by the *rows* of `pca.components_`. Previously we had been using *columns* of matrices. Once we notice that difference, everything agrees up to a sign.\n",
    "\n",
    "The `pca.singular_values_` method pulls the singular values of $X$, or the square roots of the eigenvalues of $X^T X$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca.singular_values_**2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perhaps a more useful way to compare how variance is captured by each principal vector is to look ath the *explained variance ratio*. If $\\lambda_1,\\ldots,\\lambda_d$ are the eigenvalues of $X^T X$ (listed in descending order), then the explained variance ratios are given by $\\frac{\\lambda_j}{\\sum_k \\lambda_k}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca.explained_variance_ratio_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sing = pca.singular_values_\n",
    "print(sing[0]**2/sum(sing**2),sing[1]**2/sum(sing**2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PCA as Dimension Reduction\n",
    "\n",
    "Let's return to our favorite dataset, MNIST."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_digits\n",
    "digits = load_digits()\n",
    "digits.data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can think of the MNIST dataset as a point cloud in $\\mathbb{R}^{8 \\times 8} \\approx \\mathbb{R}^{64}$. This means it is impossible to visualize directly. On the other hand, we've seen that the data is structured enough that simple classifiation algorithms like logistic regression and SVM work extremely well. This leads us to believe that we may be able to get some sort of visualization of the data by exploiting its special structure.\n",
    "\n",
    "We begin by applying PCA to MNIST."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA()\n",
    "pca.fit(digits.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pca.components_.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we look at the explained variance ratios, we observe the following. Even though the data lives in $64$ dimensions, if we use the principal vector basis then any direction after the first 10 contributes less than 2\\% of the total variance in the data!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.plot(pca.explained_variance_ratio_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we only used the first couple of principal vectors as our coordinate axes, we might actually get a reasonable picture of the MNIST dataset!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pVec0 = pca.components_[0]\n",
    "pVec1 = pca.components_[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pVec0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Even though each principal vector lives in $\\mathbb{R}^{64}$, the first two of them still span a 2-dimensional plane. We can orthogonally project each point from the MNIST dataset onto this 2-dimensional plane. This is accomplished by taking dot products with each of the principal vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "projectedMNIST = []\n",
    "\n",
    "for j in range(1797):\n",
    "    projectedMNIST.append([np.dot(digits.data[j],pVec0),np.dot(digits.data[j],pVec1)])\n",
    "\n",
    "projectedMNIST = np.array(projectedMNIST)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can plot the projected point cloud on this two dimensional plane."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,10))\n",
    "\n",
    "plt.scatter(projectedMNIST[:, 0], projectedMNIST[:, 1],\n",
    "            c=digits.target, alpha=0.5,\n",
    "            cmap=plt.cm.get_cmap('nipy_spectral_r', 10))\n",
    "\n",
    "plt.xlabel('pVec0')\n",
    "plt.ylabel('pVec1')\n",
    "plt.axis('equal')\n",
    "plt.colorbar();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This gives a relatively faithful picture of the 64-dimensional MNIST dataset in only 2-dimensions. We can see the separation in the classes and this makes it more clear why logistic regression and SVM did such a good job of separating the data.\n",
    "\n",
    "The procedure of finding a low-dimensional representation of high-dimensional data is called *dimension reduction*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Of course this dimension reduction procedure is built into `scikit-learn`. The same effect is achieved by the following code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components = 2)  # project from 64 to 2 dimensions\n",
    "projected = pca.fit_transform(digits.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from matplotlib.pyplot import figure\n",
    "\n",
    "plt.figure(figsize = (10,10))\n",
    "\n",
    "plt.scatter(projected[:, 0], projected[:, 1],\n",
    "            c=digits.target, alpha=0.5,\n",
    "            cmap=plt.cm.get_cmap('nipy_spectral_r', 10))\n",
    "\n",
    "plt.xlabel('component 1')\n",
    "plt.ylabel('component 2')\n",
    "plt.axis('equal')\n",
    "plt.colorbar();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing Variability in the Digits\n",
    "\n",
    "We can try to visualize what the principal directions are capturing in MNIST. We begin by pulling out all of the digits labeled '0'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zeroList = digits.target==0\n",
    "zeros = digits.data[zeroList]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can compute the 'average zero' and visualize it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mu = []\n",
    "for j in range(64):\n",
    "    mu.append(np.mean(zeros[:,j]))\n",
    "\n",
    "plt.imshow(np.array(mu).reshape(8,8),cmap = 'gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is pretty typical for this sort of image data: the 'average' in the label class is a very symmetric representation of the class.\n",
    "\n",
    "Next we perform PCA and get a feel for how variability is captured by the principal directions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pca = PCA()\n",
    "pca.fit(zeros)\n",
    "\n",
    "print(pca.singular_values_[:10])\n",
    "\n",
    "plt.plot(pca.explained_variance_ratio_);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we can take a look at the variability along the first principal direction as follows. We center the average zero, then move incrementally along the first principal direction. The increments should be proportional to the first singular value. This is accomplished by the following code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Center the mean, store first principal vector and singular value\n",
    "zeros_centered = zeros - np.array(mu)\n",
    "pVec1 = pca.components_[0]\n",
    "pVal1 = pca.singular_values_[0]\n",
    "\n",
    "# Create a list of offsets, proportional to the first singular value.\n",
    "offset_list = [-pVal1, -0.5*pVal1, -0.25*pVal1, -0.1*pVal1, 0*pVal1, 0.1*pVal1, 0.25*pVal1, 0.5*pVal1, pVal1]\n",
    "\n",
    "# Create a figure displaying the result of moving the average along the principal direction.\n",
    "fig = plt.figure(figsize=(20,5))\n",
    "\n",
    "for j in range(len(offset_list)):\n",
    "    offsetMu = np.array(mu) + offset_list[j]*pVec1\n",
    "    fig.add_subplot(1,len(offset_list),j+1)\n",
    "    plt.imshow(offsetMu.reshape(8,8), cmap='gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise\n",
    "\n",
    "Create similar plots to explore variability along the second principal direction for the zero digits. Does the result have any interesting interpretation.\n",
    "\n",
    "Try similar experiments for the other digit classes. Do you notice any trends?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise\n",
    "\n",
    "Here is another classification scheme: compute the mean digit in each digit class. Then use 'distance to the mean' as your classifier: given a digit vector $\\vec{x}$, label it by the digit whose mean it is closest to. How does the classification rate compare to our other methods?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise\n",
    "\n",
    "Create a 3D scatterplot showing the projection of the MNIST data onto the first 3 principal directions. How does it compare to the 2D projection?\n",
    "\n",
    "Hints:\n",
    "- Look here for an idea of how to create 3D scatterplots https://matplotlib.org/2.1.1/gallery/mplot3d/scatter3d.html\n",
    "- Use the command `%matplotlib notebook` to get a 3D plot which you can drag to rotate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise\n",
    "\n",
    "Create \"random digits\" as follows. Create a data matrix containing only the vectors for a fixed choice of digit in MNIST. Center the data matrix then compute its covariance matrix. Use this as the covariance matrix in the multivariate normal sampling function (this was used above to create examples). A sampled point is your 'random digit'. You can visualize it by reshaping and using `plt.imshow`.\n",
    "\n",
    "Try classifying your random digits with a classifier that you train on MNIST. How often are they correctly classified?\n",
    "\n",
    "**Remark:** When I tried this my random digits looked pretty bad, although they were still recognizable. Can you find a way to improve this algorithm to produce nicer looking results?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise\n",
    "\n",
    "Apply similar dimension reduction techniques on the `fashion-mnist` and/or `olivetti_faces` datasets that we have studied previously. Are these datasets similarly well-separated?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
