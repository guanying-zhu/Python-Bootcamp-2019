{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# High Dimensional Data\n",
    "\n",
    "Principal Component Analysis (PCA) is a technique for extracting information about variability in a dataset. It can be used on high-dimensional data as a form of *dimension reduction*.\n",
    "\n",
    "High-dimensional data often behaves in a way which doesn't agree with our intuition coming from 1,2 and 3-dimensional geometry. Before beginning our study of PCA, let's explore one of these counterintuitive properties, called the *concentration of measure phenomenon* https://en.wikipedia.org/wiki/Concentration_of_measure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import packages\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Write a function to uniformly sample npoints on a unit sphere sitting in R^{dim}\n",
    "def sample_spherical(npoints, dim):\n",
    "    vec = np.random.randn(dim, npoints)\n",
    "    vec /= np.linalg.norm(vec, axis=0)\n",
    "    vec = np.transpose(vec)\n",
    "    return vec\n",
    "\n",
    "# Spherical distance is given by the following function\n",
    "def spherical_dist(p1,p2):\n",
    "    if np.linalg.norm(p1-p2) < 1e-10:\n",
    "        d = 0\n",
    "    else:\n",
    "        d = np.arccos(np.dot(p1,p2))\n",
    "    return d\n",
    "\n",
    "# Given a bunch of samples S on a sphere, we compute the list of all pairwise distances\n",
    "# We don't care about distances between a point and itself\n",
    "# We are also careful to not 'double count', since distance is symmetric.\n",
    "def distance_list(S):\n",
    "    N = S.shape[0]\n",
    "    dist = [spherical_dist(S[0],S[j]) for j in range(1,N)]\n",
    "    for j in range(1,N):\n",
    "        dist = dist + [spherical_dist(S[j],S[k]) for k in range(j+1,N)]\n",
    "    return dist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise\n",
    "\n",
    "Pick an ambient dimension `dim`, sample 100 points on the unit sphere of that dimension, then plot a histogram of the pairwise distances, with the x-axis always running over all possible pairwise distances $[0,\\pi]$. \n",
    "\n",
    "Try this for several 'low' dimensions, then several 'high' dimensions. What do you notice?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dim =  # Pick a dimension here\n",
    "\n",
    "S = sample_spherical(100,dim)\n",
    "\n",
    "dist = distance_list(S)\n",
    "\n",
    "plt.hist(dist, bins=100, range=[0, np.pi], align='mid')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Principal Component Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We begin by creating some toy data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "N = 500 # Number of points to sample\n",
    "m = 0.5 # Slope\n",
    "b = 2   # Intercept\n",
    "x_shift = 4 # Horizontal shift\n",
    "\n",
    "# Sample x values according to a normal distribution, shift them horizonatally and sort them\n",
    "xs = np.random.normal(0,1,N) + x_shift\n",
    "xs = np.sort(xs)\n",
    "\n",
    "# Get y values by applying a linear map to the x values then adding noise\n",
    "ys = m*xs + b + np.random.normal(0,0.5,N)\n",
    "plt.axis('equal')\n",
    "\n",
    "plt.scatter(xs,ys)\n",
    "plt.axis('equal');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have a *point cloud*; this is common terminology for a collection of vectors in some vector space.  Intuitively, there is one direction which explains most of the variation in the data (a vector in the direction of the best fit line). The rest of the variation is in the \"thickness\" of the point cloud, which we can visualize as a shorter vector orthogonal to the first one. The goal of *principal component analysis (PCA)* is to determine these vectors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PCA: Basic Theory\n",
    "\n",
    "Let's make our goal precise. We will organize our data as a \"tall, skinny matrix\" $X \\in \\mathbb{R}^{N \\times d}$, with each of the $N$ ($N=500$ in our example) rows giving a vector in $\\mathbb{R}^d$ ($d = 2$ in our example). This is called the *data matrix*. Denote the rows by $\\vec{x}_1,\\ldots,\\vec{x}_N$, each $\\vec{x}_j \\in \\mathbb{R}^d$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array([xs,ys]).T # Form a \"tall, skinny matrix\" containing the data\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is standard to assume that the columns of $X$ have mean zero. This loses no generality, because we can preprocess by shifting, then shift back at the end if we want to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mu0 = np.mean(X[:,0])\n",
    "mu1 = np.mean(X[:,1])\n",
    "\n",
    "X_centered = X - np.array([mu0,mu1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(X[:,0],X[:,1])\n",
    "plt.scatter(X_centered[:,0],X_centered[:,1])\n",
    "plt.axis('equal');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first goal is to find a unit vector which lines up as closely with as many of the $\\vec{x}_j$'s as it possibly can. That is, we want to find \n",
    "$$\n",
    "\\vec{v}_1 = \\mathrm{argmax}_{\\|\\vec{v}\\| = 1} \\sum_j (\\vec{x}_j \\cdot \\vec{v})^2.\n",
    "$$\n",
    "This defines our first *principal vector*. \n",
    "\n",
    "To get the next principal vector, we look for the direction of greatest variation which is orthogonal to $\\vec{v}_1$. More precisely, the second principal vector $\\vec{v}_2$ is \n",
    "$$\n",
    "\\vec{v}_2 = \\mathrm{argmax} \\left\\{ \\sum_j (\\vec{x}_j \\cdot \\vec{v})^2 : \\|\\vec{v}\\| = 1 \\mbox{ and } \\vec{v} \\cdot \\vec{v}_1 = 0\\right\\}.\n",
    "$$\n",
    "This can be continued inductively to find all principal vectors $\\vec{v}_1,\\vec{v}_2, \\ldots, \\vec{v}_d$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Digging into the math a bit more, note that\n",
    "\\begin{align*}\n",
    "\\max_{\\|\\vec{v}\\| = 1} \\sum_j (\\vec{x}_j \\cdot \\vec{v})^2 &= \\max_{\\|\\vec{v}\\| = 1} \\left\\|X \\vec{v} \\right\\|^2 \\\\\n",
    "&= \\max_{\\|\\vec{v}\\| = 1} \\vec{v}^T X^T X \\vec{v} \\\\\n",
    "&= \\max_{\\vec{v} \\neq \\vec{0}} \\frac{\\vec{v}^T X^T X \\vec{v}}{\\|\\vec{v}\\|^2}.\n",
    "\\end{align*}\n",
    "It is not hard to show that this max value is the maximum eigenvalue of $X^T X$, realized by the corresponding (unit)  eigenvector. Similarly, the remaining singular vectors are obtained as the other eigenvectors of $X^T X$ (arranged in decreasing order of eigenvalue).\n",
    "\n",
    "\n",
    "\n",
    "The matrix $X^T X \\in \\mathbb{R}^{d \\times d}$ is called the *covariance matrix of $X$*.\n",
    "\n",
    "#### Conclusion\n",
    "\n",
    "The singular vectors of the centered data matrix $X$ are the unit eigenvectors (well-defined up to sign $\\pm 1$) of the covariance matrix $X^T X$, listed in descending order of corresponding eigenvalue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "cov = X_centered.T@X_centered\n",
    "cov"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can easily compute the eigenvalues of the covariance matrix using `numpy`, as we do below. The output of `np.linalg.eig` is a \"tuple\" `eVals, eVec`. The variable `eVals` stores the eigenvalues in descending order. The variable `eVec` stores the corresponding eigenvectors as **columns**. The eigenvectors are normalized (i.e., they are unit vectors)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "eVals, eVec = np.linalg.eig(cov)\n",
    "print(eVals)\n",
    "print(eVec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note the the \"slope\" of the first eigenvector is pretty close to the slope that used to construct our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eVec[1,0]/eVec[0,0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can plot the principal vectors over the pointcloud using the `quiver` function. The principal vectors are scaled by their respective eigenvalues to illustrate the difference in variability quantification. \n",
    "\n",
    "The `scale` option has been tuned to give a good picture (higher values scale the vectors down more --- note that the norms of the true principal vectors are quite large!). The same scale is used for both vectors to illustrate their relative lengths."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.scatter(X_centered[:, 0], X_centered[:, 1], alpha=0.2)\n",
    "# alpha controls 'opacity' of the points\n",
    "\n",
    "# Syntax for quiver:\n",
    "# plt.quiver(xVal for basepoint, yVal for basepoint, xVal for vector, yVal for vector, scale = )\n",
    "plt.quiver(0, 0, eVals[0]*eVec[0,0], eVals[0]*eVec[1,0], scale=1200)\n",
    "plt.quiver(0, 0, eVals[1]*eVec[0,1], eVals[1]*eVec[1,1], scale=1200)\n",
    "\n",
    "plt.axis('equal');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We could also shift everything back to the original data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(X[:, 0], X[:, 1], alpha=0.2)\n",
    "\n",
    "# Syntax for quiver:\n",
    "# plt.quiver(xVal for basepoint, yVal for basepoint, xVal for vector, yVal for vector, scale = )\n",
    "plt.quiver(mu0, mu1, eVals[0]*eVec[0,0], eVals[0]*eVec[1,0], scale=1200)\n",
    "plt.quiver(mu0, mu1, eVals[1]*eVec[0,1], eVals[1]*eVec[1,1], scale=1200)\n",
    "\n",
    "plt.axis('equal');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PCA with SVD\n",
    "\n",
    "The *Singular Value Decomposition (SVD)* expresses $X$ as \n",
    "$$\n",
    "X = U \\Sigma W^T,\n",
    "$$\n",
    "where $U \\in \\mathbb{R}^{N \\times N}$ and $W \\in \\mathbb{R}^{d \\times d}$ are matrices with orthonormal column vectors (orthogonal matrices) and $\\Sigma \\in \\mathbb{R}^{N \\times d}$ is a diagonal matrix with the square roots of the eigenvalues of $X^T X$ on its diagonal. These are called the *singular values* of $X$.\n",
    "\n",
    "We calculate the SVD of our centered data using the code below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "U, Sigma, Wt = np.linalg.svd(X_centered)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print(U.shape)\n",
    "print(Sigma.shape)\n",
    "print(Wt.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `Sigma` output is a list of singular values. If we actually want the matrix which appears in the decomposition, we can use the `fill_diagonal` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Sigma_diag = np.zeros((500,2))\n",
    "np.fill_diagonal(Sigma_diag,Sigma) # Note this function changes the value of Sigma_diag\n",
    "Sigma_diag"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can check that the really decomposition does what we claimed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Take the matrix norm of the difference between X and its SVD\n",
    "np.linalg.norm(X_centered - U@Sigma_diag@Wt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For most applications, just the list of singular values will be useful. Note that the output of the SVD function gives Sigma as a list of singular values in *descending order* of magnitude."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Sigma"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we square these values, we should get the eigenvalues of $X^T X$ that we computed earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(Sigma**2) # Note that this syntax applies the **2 operation to each entry in the array\n",
    "print(eVals)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And if we examine the matrix $W^T$, we see that its columns are (up to a sign) the same as the eigenvectors of $X^T X$ we computed earlier!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(Wt)\n",
    "print(eVec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Indeed, this must be the case in general, since\n",
    "\\begin{align*}\n",
    "X^T X &= (U\\Sigma W^T)^T (U \\Sigma W^T) \\\\\n",
    "&= W \\Sigma^T U^T U \\Sigma W^T \\\\\n",
    "&= W \\Sigma^T \\Sigma W^T \\\\\n",
    "&= W \\widehat{\\Sigma} W^T,\n",
    "\\end{align*}\n",
    "where $\\widehat{\\Sigma}$ is a diagonal matrix containing the eigenvalues of $X^T X$. It follows that the columns of $W^T$ are the eigenvectors of $X^T X$. \n",
    "\n",
    "#### Conclusion\n",
    "\n",
    "The principal vectors of $X$ are given by the columns of $W^T$ from the SVD of $X$ (read from left-to-right).\n",
    "\n",
    "### Theoretical Homework\n",
    "\n",
    "Verify the steps and statements made in the above proof."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise\n",
    "\n",
    "Write a function to compute the principal vectors of a 2-dimensional point cloud. Use it to perform PCA on the examples `X1`, `X2` and `X3` defined below. Plot the results.\n",
    "\n",
    "For plotting purposes, it may be useful to have your function return a 'tuple'; e.g.\n",
    "`return X_centered, Sigma, Wt`\n",
    "or something along those lines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Example 1 generation\n",
    "mean1 = [0, 0]\n",
    "cov1 = [[10, 0], [0, 10]]\n",
    "\n",
    "X1 = np.random.multivariate_normal(mean1, cov1, 500)\n",
    "\n",
    "# Example 2 generation\n",
    "mean2 = [2, -3]\n",
    "cov2 = [[1, 1], [1, 10]]\n",
    "\n",
    "X2 = np.random.multivariate_normal(mean2, cov2, 500)\n",
    "\n",
    "# Example 3 generation\n",
    "xs = np.random.uniform(0,2*np.pi,500)\n",
    "xs = np.sort(xs)\n",
    "ys = 2*np.sin(xs) + np.random.normal(0,0.5,500)\n",
    "X3 = np.array([xs,ys]).T\n",
    "\n",
    "fig = plt.figure(figsize=(10,5))\n",
    "\n",
    "fig.add_subplot(1,3,1)\n",
    "plt.scatter(X1[:,0],X1[:,1])\n",
    "plt.axis('equal')\n",
    "\n",
    "fig.add_subplot(1,3,2)\n",
    "plt.scatter(X2[:,0],X2[:,1])\n",
    "plt.axis('equal')\n",
    "\n",
    "fig.add_subplot(1,3,3)\n",
    "plt.scatter(X3[:,0],X3[:,1])\n",
    "plt.axis('equal')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PCA with SciKit-Learn\n",
    "\n",
    "Now that we have a solid theoretical understanding of PCA, we can use a built in function from `scikit-learn` to do the computation. This will work in arbitrary dimension, unlike the function we created above.\n",
    "\n",
    "Let's try it on the dataset `X` we have been using to make sure it agrees."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(n_components=2) \n",
    "# Specify the number of principal directions you want to find\n",
    "# It defaults to finding all of the them, so it was not necessary to include it here\n",
    "pca.fit(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check the principal vectors with `pca.components_` and compare to our earlier result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(pca.components_)\n",
    "print(eVec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Careful! We should notice that the principal vectors we are after are given by the *rows* of `pca.components_`. Previously we had been using *columns* of matrices. Once we notice that difference, everything agrees up to a sign.\n",
    "\n",
    "The `pca.singular_values_` method pulls the singular values of $X$, or the square roots of the eigenvalues of $X^T X$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca.singular_values_**2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perhaps a more useful way to compare how variance is captured by each principal vector is to look ath the *explained variance ratio*. If $\\lambda_1,\\ldots,\\lambda_d$ are the eigenvalues of $X^T X$ (listed in descending order), then the explained variance ratios are given by $\\frac{\\lambda_j}{\\sum_k \\lambda_k}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca.explained_variance_ratio_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PCA on MNIST\n",
    "\n",
    "Let's return to our favorite dataset, MNIST."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1797, 64)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.datasets import load_digits\n",
    "digits = load_digits()\n",
    "digits.data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall that the MNIST dataset consists of handwritten digits, given as $8\\times 8$ images. These can be reshaped to give a pointcloud in $\\mathbb{R}^{64}$. We can apply PCA to the MNIST vectors to study the variability in the pointcloud."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PCA(copy=True, iterated_power='auto', n_components=None, random_state=None,\n",
       "  svd_solver='auto', tol=0.0, whiten=False)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca = PCA()\n",
    "pca.fit(digits.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(64, 64)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pca.components_.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we look at the explained variance ratios, we observe the following. Even though the data lives in $64$ dimensions, if we use the principal vector basis then any direction after the first 10 contributes less than 2\\% of the total variance in the data!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x1a1ac29278>]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(pca.explained_variance_ratio_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This suggests that the $64$-dimensional MNIST dataset can potentially be explained relatively faithfully with far fewer dimensions. This idea is called *dimension reduction*, and is explored in the next notebook."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
