{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Polynomial Regression\n",
    "\n",
    "Suppose we are given datasets of explanatory variables $X = \\{x_1,\\ldots,x_n\\}$ and response variables $Y = \\{y_1,\\ldots,y_n\\}$, with (for the sake of simplicity) all $x_j$ and $y_j$ real numbers. \n",
    "\n",
    "Last time we studied *simple linear regression*, where we sought to find a function of the form\n",
    "$$\n",
    "f(x) = m x + b\n",
    "$$\n",
    "minimizing the loss function\n",
    "$$\n",
    "L(m,b) = \\sum_j (f(x_j) - y_j)^2.\n",
    "$$\n",
    "\n",
    "Suppose that we suspect our response variables are not *linearly* related to the explanatory variables, but satisfy\n",
    "$$\n",
    "y_j \\approx \\beta_0 + \\beta_1 x_j + \\beta_2 (x_j)^2 + \\cdots + \\beta_m (x_j)^m =: g(x_j),\n",
    "$$\n",
    "for some degree $m$ and some constants $\\beta_k$. We could once again look for constants minimizing the loss function\n",
    "$$\n",
    "M(\\beta_0,\\beta_1,\\ldots,\\beta_m) = \\sum_j (g(x_j) - y_j)^2\n",
    "$$\n",
    "This is called *polynomial regression*.\n",
    "\n",
    "Observe that if we treat the $x_j, x_j^2, \\ldots, x_j^m$ as features in a vector $\\vec{x}_j$, then this is exactly the setup for multiple linear regression! Therefore polynomial regression is a special type of multiple linear regression and our techniques from last time apply!\n",
    "\n",
    "We will explore some aspects of polynomial regression by applying it to some toy data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "N = 200\n",
    "noise_level = 0.2\n",
    "xs = np.random.uniform(0,np.pi,N)\n",
    "xs = np.sort(xs) # for plotting later\n",
    "ys = np.sin(xs) + np.random.normal(0,noise_level,N) # Another way to add noise to data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that `xs` and `ys` are not Python lists but rather `numpy`\n",
    "arrays.  When passing this data to `scikit-learn`, we will be happier if we reshape our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xs = xs[:, np.newaxis]\n",
    "ys = ys[:, np.newaxis]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let&rsquo;s see a plot of our random data.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.scatter(xs,ys)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now pretend that we don&rsquo;t know the source of this data, and we wish\n",
    "to &ldquo;learn&rdquo; the relationship between the $x$&rsquo;s and the $y$&rsquo;s.  Of\n",
    "course, the truth is that $y = \\sin x$ plus some noise, but let&rsquo;s\n",
    "forget about that and see what we can recover.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear regression\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We&rsquo;ve been learning about linear regression, so let&rsquo;s use\n",
    "`scikit-learn` to **again** perform linear regression on our data.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "lm = LinearRegression().fit( xs, ys )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let&rsquo;s make a plot our (linear!) model.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ys_predicted = lm.predict(xs)\n",
    "plt.scatter(xs, ys)\n",
    "plt.plot(xs, ys_predicted, color='r')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because we followed the usual advice to ****look at our data****, we know\n",
    "our data isn&rsquo;t modeled well by a straight line.  This is an example of\n",
    "**underfitting**.  We need a more complex model to capture the actual\n",
    "pattern of the data.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Polynomial Regression\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We find the &ldquo;polynomial features&rdquo; associated to the $x$&rsquo;s.  This\n",
    "replaces the vector $(x) \\in \\mathbb{R}^1$ with the vector $(1,x,x^2)\n",
    "\\in \\mathbb{R}^3$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "polynomial_features= PolynomialFeatures(degree=2)\n",
    "xs_poly = polynomial_features.fit_transform(xs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now (since this is a special example of multiple linear regression), we fit a linear regression model to the adjusted data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qm = LinearRegression().fit( xs_poly, ys )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let&rsquo;s plot the data as a scatterplot, and our model&rsquo;s predicted values\n",
    "as a red curve.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ys_predicted = qm.predict(xs_poly)\n",
    "plt.scatter(xs, ys)\n",
    "plt.plot(xs, ys_predicted, color='r')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That **looks** much better.  Is it &ldquo;actually&rdquo; better?\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"linear model score:\",lm.score(xs,ys))\n",
    "print(\"quadratic model score:\",qm.score(xs_poly,ys))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overfitting\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise \n",
    "\n",
    "If degree 2 worked well, surely degree 25 is even better! Fit a degree-25 polynomial to the data and determined the $R^2$ score of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that our new model fits even better! It even captures some little bumps in the data! \n",
    "\n",
    "Actually, this is an example of *overfitting*, where the model fitting the noise in the data. To quantify how robustly our models are really fitting, we can use cross validation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Homework\n",
    "\n",
    "Use cross-validation to determine the appropriate degree for polynomial regression on this data.\n",
    "\n",
    "**Remark:** In the cross-validation experiments last class, our datasets were stored as dataframes. If you use the code from last class as a starting point, this will have to be addressed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression\n",
    "\n",
    "Like linear regression, logistic regression seeks a function which models a given data set as faithfully as possible. A main distinction is that logistic regression treats *discrete* response variables, which we refer to as *labels*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Motivating Example\n",
    "\n",
    "Recall the `iris` dataset, containing biological measurements for samples of a few different flower species. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import seaborn as sns # sns is the standard abbreviation for seaborn\n",
    "\n",
    "iris = sns.load_dataset(\"iris\")\n",
    "iris.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset contains flowers from three different species, as we can see below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris.species.unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's subset the dataframe to get a dataframe containing only two of the species."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df = iris[(iris.species == 'setosa') | (iris.species == 'versicolor')]\n",
    "# The vertical line | is read as \"or\".\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, let's create a scatterplot of all points in this new dataframe, with 'petal_length' on the $x$-axis and 'petal_width' on the $y$-axis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "sns.scatterplot(x=\"petal_length\", y=\"petal_width\",\n",
    "                hue=\"species\", data=df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We observe that the species are quite separated in this scatterplot. It stands to reason that we should be able to predict the species of a flower based on its petal lenth and width.\n",
    "\n",
    "Of course, the method of prediction that we used last time (linear regression) doesn't make sense here. Our response variables are labels 'setosa' and 'versicolor', rather than real numbers!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression Problem Setup\n",
    "\n",
    "\n",
    "In the *logistic regression* problem, we are given data consisting of:\n",
    "\n",
    "- a set of *explanatory variables* $X = \\{\\vec{x}_1,\\ldots,\\vec{x}_n\\}$, each $\\vec{x}_j \\in \\mathbb{R}^d$\n",
    "\n",
    "- a set of *response variables* $y = \\{y_1,\\ldots,y_n\\}$. Now our response variables are *discrete*, say $y_j \\in \\{0,1\\}$ for all $j$. In this setting they $y_j$'s are called labels.\n",
    "\n",
    "- we seek a function $f$ such that $f(\\vec{x}_j) = y_j$ for each $j$. \n",
    "\n",
    "Achieving the goal may not be possible (the labels may not be completely determined by the features). In any case, we need to formulate this in a way where there is some hope of finding a solution (i.e., amenable to gradient descent).\n",
    "\n",
    "We fix a class of functions of the form\n",
    "$$\n",
    "f(\\vec{x}) = \\frac{1}{1 + \\exp(-g(\\vec{x}))},\n",
    "$$\n",
    "with \n",
    "$$\n",
    "g(\\vec{x}) = \\beta_1 x^1 + \\cdots + \\beta_d x^d + b.\n",
    "$$\n",
    "The goals is then to find the weights $\\beta_1,\\ldots,\\beta_d,b$ which minimize the *loss function*\n",
    "$$\n",
    "L(\\beta_1,\\ldots,\\beta_d,b) = - \\frac{1}{n} \\sum_{j=1}^n \\Big[ y_j \\log(f(\\vec{x}_j)) + (1 - y_j) \\log(1 - f(\\vec{x}_j)) \\Big]\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise: Why is This a Reasonable Thing to Do?\n",
    "\n",
    "Let's pause for a moment to see why this makes sense. The *sigmoid function* is given by\n",
    "$$\n",
    "S(x) = \\frac{1}{1 + \\exp(-x)}.\n",
    "$$\n",
    "So we are considering functions of the form\n",
    "$$\n",
    "f(\\vec{x}) = S(g(\\vec{x})),\n",
    "$$\n",
    "where $g$ is a linear function as defined above. \n",
    "\n",
    "Plot the sigmoid function over the interval $[-10,10]$. Does it make sense that this would be useful for classifying binary variables? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some potentially useful functions\n",
    "from math import exp, log\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise, Continued\n",
    "\n",
    "Now consider the loss function. The goal of the loss function should be to 'penalize' wrong guesses. So the summand\n",
    "$$\n",
    "-\\big[y_j \\log(f(\\vec{x}_j)) + (1 - y_j) \\log(1 - f(\\vec{x}_j))\\big]\n",
    "$$\n",
    "should be:\n",
    "- *small* if $y_j = 1$ and $f(\\vec{x}_j) \\approx 1$ or $y_j = 0$ and $f(\\vec{x}_j) \\approx 0$;\n",
    "- *large* if $y_j = 1$ and $f(\\vec{x}_j) \\approx 0$ or $y_j =0$ and $f(\\vec{x}_j) \\approx 1$.\n",
    "\n",
    "Try a few 'artificial examples' to verify that this expression behaves the way that we want  (i.e., artificially pick values of $f(\\vec{x}_j)$, $y_j$ and evaluate the expression)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solving the Logistic Regression Problem\n",
    "\n",
    "Unlike linear regression, the logistic regression problem has no explicit solution. Nonetheless, one can check that the loss function is *convex*---this is a mathematical condition which guarantees the loss function has a unique minimum (an example of a convex function is $h(x) = x^2$).\n",
    "\n",
    "This minimum can be (and typically *is*) found by gradient descent (or some variation of it). The gradient descent function we studied last class could be modified to work here. \n",
    "\n",
    "Of course, logistic regression is built into packages such as `scikit-learn`. The computation of the gradient in this case is a bit more involved, so let's skip on coding this ourselves and just use an existing function! The problem setup that is used by `scikit-learn` is a bit different than what was described above (e.g., they use a 'regularized' cost function), but is morally the same thing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression on the Iris Dataset\n",
    "\n",
    "Let's apply logistic regression to see if we can classify flower species by petal length and width. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import some useful functions\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's get our data. The explanatory variables are 'petal_length' and 'petal_width' (so there are two features, and each element of our set $X$ is a vector in $\\mathbb{R}^2$). We also need to get the set $y$ consisting of labels. Let's set the species 'setosa' to have label 1 and 'versicolor' to have label 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df[['petal_length','petal_width']] # We only want column values for these variables\n",
    "y = df.species == 'setosa' # First construct a boolean sequence\n",
    "y = [int(val) for val in y] # Set 'True' to 1, 'False' to 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To test the performance of logistic regression for this classification problem, we will split our data into a 'testing set' and a 'training set'. We have already imported a function which will do this automatically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct the training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=1)\n",
    "\n",
    "# Check the sizes of the sets\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)\n",
    "print(len(y_train))\n",
    "print(len(y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will fit the logistic regression model to our training data. The optimization is done using a fancier method than gradient descent https://en.wikipedia.org/wiki/Limited-memory_BFGS. It is still an iterative approach (like gradient descent), which is killed at 10,000 iterations if necessary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LogisticRegression(solver='lbfgs', max_iter=10000)\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can take a look at the coefficients. These are the $\\beta_1$, $\\beta_2$ and $b$ from the description above, respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model.coef_, model.intercept_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we have found that the optimal classification function is given by\n",
    "$$\n",
    "f(\\vec{x}) = \\frac{1}{1 + \\exp(2.384 x^1 + 1.016 x^2 - 7.338)}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The logistic regression model can be used to predict the species of a flower in the testing set. Let's look at the first few examples in the testing set. We see that the first two are 'versicolor' (label 0) and the last is 'setosa' (label 1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(X_test[0:3])\n",
    "print(y_test[0:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We predict the label of a sample by applying the function $f$ shown above to the 'petal_length' and 'petal_width' numbers. If $f$ returns a value less than 0.5, we classify the sample as 0, or 'versicolor'. If $f$ returns a value greater than 0.5, we classify the sample as 1, or 'setosa'.\n",
    "\n",
    "This can be performed using the `predict` attribute of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.predict(X_test[0:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So our model mades some correct predictions! This should not be too surprising, since the data was pretty well separated in the plot above. In fact, we can see that our model must do a pretty good job by plotting the function $f$ written above on each sample in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the function f\n",
    "def f(x1,x2):\n",
    "    return 1/(1+exp(2.384*x1+1.016*x2-7.338))\n",
    "\n",
    "# Turn the data into an np array for convenience\n",
    "X2 = np.array(X)\n",
    "# Apply f to all of the samples\n",
    "ys = [f(x[0],x[1]) for x in X2]\n",
    "\n",
    "# Plot the y values\n",
    "plt.plot(ys)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By the way, the above could have also been done directly using `scikit-learn` functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(model.predict_proba(X)[:,1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This looks like a pretty convincing split between the flower species. To be safe, we can double check the classification scores of the model on the testing and training sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(model.score(X_train,y_train))\n",
    "print(model.score(X_test,y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is calculated as follows. For each record in, say, `X_train`, we assign a label via the classification function $f$. We compare this to the correct label from `y_train`, count the number of correct labels across all samples, then divide by the number of samples. So our model perfectly classifies everything. \n",
    "\n",
    "Once again, this is not too surprising since the data was so well separated..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A Harder Example\n",
    "\n",
    "Let's generate some toy data to make sure that logistic regression isn't \"too good to be true\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_blobs\n",
    "\n",
    "X, y = make_blobs(n_samples= 1000, centers=2, center_box = [-2,2], random_state=1)\n",
    "# Play with the parameters above to try different examples.\n",
    "\n",
    "fig = plt.figure(figsize=(8,6))\n",
    "plt.scatter(X[:,0], X[:,1], c=y)\n",
    "plt.title(\"Dataset\")\n",
    "plt.xlabel(\"First feature\")\n",
    "plt.ylabel(\"Second feature\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise\n",
    "\n",
    "Replicate the above experiment on this toy data (split into train/test data, train a model, determine its performance). Try varying the parameters in the toy data creation to see how logistic regression performs in different circumstances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise\n",
    "\n",
    "In the `iris` dataset, the species 'versicolor' and 'virginica' seem to be not as well separated. Train/test a regression model to distinguish these species. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise\n",
    "\n",
    "Load the `mpg` dataset from `seaborn`. See if you can predict whether a car was made in the USA using logistic regression. Which explanatory variables have the biggest effect on classification score?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise \n",
    "\n",
    "One great source of both data **and** community is Kaggle.\n",
    "\n",
    "Go to [https://www.kaggle.com/c/titanic/data>](https://www.kaggle.com/c/titanic/data>)to download data\n",
    "describing passengers from the Titanic, and whether or not they\n",
    "survived their voyage.  Can you use logistic regression (or any other\n",
    "techniques!) to predict who surived?  (Perhaps &ldquo;predict&rdquo; is something\n",
    "of a misnomer since the Titanic has already sunk!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
