{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regularization\n",
    "\n",
    "The methods that we have seen so far --- linear/polynomial regression for predicting continuous response variables and logistic regression for predicting (discrete) labels --- are called *supervised learning* algorithms because the models are trained using correctly labeled data. Today we will put this in a general context. We first explore the effect of *regularizing* our algorithms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regularizing Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's return to a toy example we've used previously, a noisy sine curve. For this example, let's use a more sparse sampling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "N = 25\n",
    "X = np.random.uniform(0,np.pi,N) # Sample the x-axis uniformly\n",
    "X = np.sort(X) # Useful for plotting later\n",
    "y = np.sin(X) + np.random.normal(0,0.1,N) # Create a noisy sine curve\n",
    "\n",
    "# Put the data into a form that sk-learn will like\n",
    "X = X[:, np.newaxis]\n",
    "y = y[:, np.newaxis]\n",
    "\n",
    "plt.scatter(X,y)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data looks like a good candidate for polynomial regression. Remember, this is just a special case of linear regression, where we preprocess by creating feature vectors of the form $\\vec{x}_j = (x_j,x_j^2,\\ldots,x_j^d)$, where $d$ is the degree of the polynomial we will fit. \n",
    "\n",
    "Let's go overboard and fit a degree-10 polynomial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "polynomial_features= PolynomialFeatures(degree=10)\n",
    "X_poly = polynomial_features.fit_transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's split the data into a training and a testing set to see how well this method of regression performs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_poly, y, test_size = 0.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We fit a model using the training data.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "model = LinearRegression().fit( X_train, y_train )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It fits well on the training data!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.score( X_train, y_train )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not so well on the testing set.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.score( X_test, y_test )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise\n",
    "\n",
    "Plot the polynomial on the same plot with the training data scatterplot and the testing data scatterplot (use different colors for the scatterplots).\n",
    "\n",
    "Does the discrepancy between these fitting scores make sense?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ridge Regularization\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perhaps unsurprisingly, when fitting a high-degree polynomial, we&rsquo;re\n",
    "overfitting. \"Regularization\" is helpful here.\n",
    "\n",
    "Our usual linear regression optimizes the *sum of squared residuals* loss function\n",
    "$$\n",
    "L(b, \\beta_1,\\ldots,\\beta_d) = \\frac{1}{2n}\\sum_j \\left((b + \\beta_1 x_j + \\beta_2 x_j^2 + \\cdots + \\beta_d x_j^d) - y_j\\right)^2.\n",
    "$$\n",
    "The *Ridge regression algorithm* adds a *regularization* term to the loss function. We optimize\n",
    "$$\n",
    "\\widetilde{L}(b, \\beta_1, \\ldots, \\beta_d) = L(b,\\beta_1,\\ldots,\\beta_d) + \\alpha \\left(b^2 + \\beta_1^2 + \\dots \\beta_d^2\\right),\n",
    "$$\n",
    "where $\\alpha$ is a parameter to be chosen.\n",
    "\n",
    "This is interpreted as penalizing the size of the weights in the polynomial. In other words, a more complex model incurs a higher cost. This promotes the optimization procedure to find a simpler model to fit the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Ridge\n",
    "model_ridge = Ridge(alpha=1).fit( X_train, y_train )\n",
    "# The default alpha is 1, so we didn't need to include it here. \n",
    "# It's included to just to illustrate the syntax."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It regularized model does worse on the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_ridge.score( X_train, y_train )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But better on the testing set.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model_ridge.score( X_test, y_test )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comparing the coefficients, we see that the ridge regression coefficients are indeed much smaller in magnitude!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(model.coef_)\n",
    "print(model_ridge.coef_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the model which was fit by ridge regression doesn't have the huge deviation that came from fitting the noise without regularization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xs = np.linspace(0,np.pi,100)\n",
    "xs = xs[:,np.newaxis]\n",
    "xs_poly = polynomial_features.fit_transform(xs)\n",
    "\n",
    "ys_predict = model_ridge.predict(xs_poly)\n",
    "plt.plot(xs,ys_predict)\n",
    "\n",
    "plt.scatter(X_train[:,1],y_train)\n",
    "plt.scatter(X_test[:,1],y_test)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Choosing alpha\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let&rsquo;s see how the fit depends on the choice of $\\alpha$.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Ridge\n",
    "\n",
    "alphas = np.linspace(0.00001,1,100)\n",
    "scores = []\n",
    "for alpha in alphas:\n",
    "    model = Ridge(alpha=alpha).fit( X_train, y_train )\n",
    "    scores.append( model.score( X_test, y_test ) )\n",
    "\n",
    "plt.scatter( alphas, scores )\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aside: Improving Notation\n",
    "\n",
    "At this point, it makes sense to switch to common notation involving matrix expressions for the loss functions. For the general multiple linear regression problem, our data takes the form $X = \\{\\vec{x}_1,\\ldots,\\vec{x}_n\\}$, with $\\vec{x}_j = (x_j^1,\\ldots,x_j^d) \\in \\mathbb{R}^d$, and $y = \\{y_1,\\ldots,y_n\\}$ with $y_j \\in \\mathbb{R}$. \n",
    "\n",
    "Let \n",
    "$$\n",
    "X = \\left(\\begin{array}{ccccc}\n",
    "1 & x_1^1 & x_1^2 & \\cdots & x_1^d \\\\\n",
    "1 & x_2^1 & x_2^2 & \\cdots & x_2^d \\\\\n",
    "\\vdots & \\vdots & \\vdots & & \\vdots \\\\\n",
    "1 & x_n^1 & x_n^2 & \\cdots & x_n^d \\end{array}\\right) = \\left(\\begin{array}{cc}\n",
    "1 & \\vec{x}_1^T \\\\\n",
    "1 & \\vec{x}_2^T \\\\\n",
    "\\vdots & \\vdots \\\\\n",
    "1 & \\vec{x}_n^T \\end{array}\\right) \\in \\mathbb{R}^{n \\times (d+1)}\n",
    "$$\n",
    "denote the *matrix of features*. Similarly, let\n",
    "$$\n",
    "\\vec{w} = \\left(\\begin{array}{c}\n",
    "b \\\\\n",
    "\\beta_1 \\\\\n",
    "\\vdots \\\\\n",
    "\\beta_d \\end{array}\\right) \\in \\mathbb{R}^{d+1}\n",
    "$$\n",
    "denote the *vector of weights* and \n",
    "$$\n",
    "\\vec{y} = \\left(\\begin{array}{c}\n",
    "y_1 \\\\\n",
    "\\vdots \\\\\n",
    "y_n \\end{array}\\right) \\in \\mathbb{R}^n\n",
    "$$\n",
    "the *vector of response variables*. \n",
    "\n",
    "Then\n",
    "$$\n",
    "X \\vec{w} - \\vec{y} = \\left(\\begin{array}{c}\n",
    "b + \\beta_1 x_1^1 + \\cdots + \\beta_d x_1^d - y_1 \\\\\n",
    "b + \\beta_1 x_2^1 + \\cdots + \\beta_d x_2^d - y_2 \\\\\n",
    "\\vdots \\\\\n",
    "b + \\beta_1 x_n^1 + \\cdots + \\beta_d x_n^d - y_n\n",
    "\\end{array}\\right).\n",
    "$$\n",
    "\n",
    "\n",
    "Therefore the loss function for (unregularized) linear regression above can be written more simply as\n",
    "$$\n",
    "L(\\vec{w}) = \\frac{1}{2n} \\|X\\vec{w} - \\vec{y}\\|^2\n",
    "$$\n",
    "and the loss function for ridge regression becomes\n",
    "$$\n",
    "\\widetilde{L}(\\vec{w}) = \\frac{1}{2n} L(\\vec{w}) + \\alpha \\|\\vec{w}\\|^2.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LASSO Regression\n",
    "\n",
    "We generate some new toy data below. Most of the toy data that we have used so far has been two or three-dimensional, allowing us to easily visualize it. In the real world we frequently deal with **high**-dimensional data. The data we generate below is 25-dimensional (by default, although this parameter can be changed)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "dimension = 25 # should be a perfect square\n",
    "N = 1000\n",
    "\n",
    "y = np.random.normal( 0, 1, size=N )\n",
    "\n",
    "vs = np.linspace(0.01,2,dimension)\n",
    "np.random.shuffle(vs)\n",
    "\n",
    "xs = []\n",
    "for v in vs:\n",
    "    xs.append( np.random.normal(0,1) * np.random.normal( y, v ) )\n",
    "X = np.array( xs ).transpose()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we have $N$ ($1000$ by default) data points $(\\vec{x}_j,y_j)$. Each $x_j$ is $25$-dimensional (by default)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take a look at one of the feature vectors\n",
    "X[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# And the corresponding response value\n",
    "y[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To visualize a scatterplot of $(\\vec{x}_j,y_j)$'s, we would need to plot a point cloud in $26$ dimensions. Of course, this is impossible. We can get some sense of the data by making a scatterplot of the $y_j$'s against each coordinate of the $\\vec{x}_j$'s."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "grid = int(np.sqrt(dimension))\n",
    "for i in range(dimension):\n",
    "    plt.subplot(grid, grid, 1+i)\n",
    "    plt.scatter(X[:,i], y, s=1 )\n",
    "    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So the data is fairly complex, but we've constructed it to have lots of colinear structure. Let's perform linear regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "model = LinearRegression().fit( X, y )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our model fits very well!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.score(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But it is arguably upsetting that the model coefficients are involving **all** the features of the data.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.coef_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LASSO (Least Absolute Shrinkage and Selection Operator)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our usual linear regression is the cost function\n",
    "$$\n",
    "\\frac{1}{2n}\\|X \\vec{w} - \\vec{y}\\|^2.\n",
    "$$\n",
    "\n",
    "For *LASSO* we instead minimize the cost function\n",
    "$$\n",
    "\\frac{1}{2n} \\|X \\vec{w} - \\vec{y}\\|_2 ^ 2 + \\alpha \\|\\vec{w}\\|_1.\n",
    "$$\n",
    "Once again, $\\alpha$ is the regularization parameter. \n",
    "\n",
    "Recall that if $\\vec{w} = (w_1,\\ldots,w_m)$, then the *$\\ell_1$-norm* is given by\n",
    "$$\n",
    "\\|\\vec{w}\\|_1 = |w_1| + |w_2| + \\cdots + |w_m|.\n",
    "$$\n",
    "Regularizing with $\\ell_1$ penalty (i.e., adding a $\\|\\cdot\\|_1$ term to the loss function) is *sparsity-promoting*; it favors weight vectors with lots of zero entries.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Lasso\n",
    "model = Lasso(alpha=1).fit( X, y )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Does this really change the coeffiicients at all?\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "model.coef_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that LASSO really has promoted sparsity in the coefficients. This is useful for explainability. This suggests that the response variables are really controlled for the most part by only a *few* of the features, rather than *all* of them, as unregularized linear regression might suggest."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise\n",
    "\n",
    "Apply LASSO to the previous example (the noisy sine curve --- unless you changed something, the train/test data from that example should still be assigned to the relevant variables). You may need to adjust parameters; see https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Lasso.html.\n",
    "\n",
    "What effect does it have on classification rate compared to ridge regression? Do the resulting coefficients make sense? Plot the fitting polynomial, like we did above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regularizing Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look again at the MNIST data to see the effect of regularization on Logistic regression. We first import the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "digits = datasets.load_digits()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall that the data consists of many handwritten digits and their correct labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(20,10))\n",
    "\n",
    "for j in range(10):\n",
    "    fig.add_subplot(2,5,j+1)\n",
    "    plt.imshow(digits.images[j], cmap='gray')\n",
    "    # imshow is a useful function. \n",
    "    # It treats an array of numbers as an image, with the number in each entry \n",
    "    # corresponding to a color."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can treat each image as a vector in $\\mathbb{R}^{8 \\times 8} \\approx \\mathbb{R}^{64}$. Conveniently, `digits.data` reshapes each image into a 64 dimensional vector. This allows us to perform multiclass logistic regression to classify the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split \n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "MNIST = digits.data\n",
    "MNISTlabels = digits.target\n",
    "\n",
    "MNIST_train, MNIST_test, MNISTlabels_train, MNISTlabels_test = train_test_split(MNIST, MNISTlabels, random_state=1)\n",
    "\n",
    "model = LogisticRegression(solver='lbfgs', multi_class = 'multinomial', max_iter=10000)\n",
    "model.fit(MNIST_train, MNISTlabels_train)\n",
    "\n",
    "print(model.score(MNIST_train,MNISTlabels_train))\n",
    "print(model.score(MNIST_test,MNISTlabels_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logistic regression performs extremely well! Once the classification model has been created, we have a list of coefficients for each label. Let's try to visualize what those coefficients are telling us about our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(20,10))\n",
    "\n",
    "for j in range(10):\n",
    "    fig.add_subplot(2,5,j+1)\n",
    "    plt.imshow(model.coef_[j].reshape(8,8), cmap='gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## $\\ell_1$ Regularization\n",
    "\n",
    "The `scikit-learn` logistic regression solver doesn't actually minimize the cross entropy function, but an $\\ell_2$-regularized version. In the $2$-label setting, the loss function looks like\n",
    "$$\n",
    "\\widehat{L}\\left(\\vec{w}\\right) = L\\left(\\vec{w}\\right) + \\|\\vec{w}\\|^2 ,\n",
    "$$\n",
    "where $L(\\vec{w})$ is our usual loss function for multiclass logistic regression. As in the case of linear regression, this penalizes large weights, which can help to avoid overfitting in the model.\n",
    "\n",
    "Different types of regularization can be used. For example, the following code fits a model for the MNIST dataset with $\\ell_1$ regularization, which has loss of the form\n",
    "$$\n",
    "\\widetilde{L}\\left(\\vec{w}\\right) =  L\\left(\\vec{w}\\right) + \\|\\vec{w}\\|_1.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise\n",
    "\n",
    "The code below will use $\\ell_1$ regularization to fit a logistic regression model on MNIST. Fit the model, then study the effect. Does it change classification rate? How are the weight coefficients affected by the choice of regularization? Does this match your intuition coming from the linear regression example?\n",
    "\n",
    "Note that I named the model `modelL1` so it won't overwrite the previous model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "modelL1 = LogisticRegression(solver='saga', multi_class = 'multinomial', max_iter=10000, penalty='l1')\n",
    "modelL1.fit(MNIST_train, MNISTlabels_train)\n",
    "#Note that we have to change the solver to `saga` to use this regularization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A General Supervised Learning Framework\n",
    "\n",
    "All of the methods that we have studied fit more-or-less into a common framework for *parametric supervised learning*.\n",
    "\n",
    "#### Training Data \n",
    "\n",
    "Our training data is given as $X = \\{\\vec{x}_1,\\ldots,\\vec{x}_n\\}$ and $Y = \\{y_1,\\ldots,y_n\\}$, where $\\vec{x}_j \\in \\mathbb{R}^d$ and $y_j$ can be continuously-valued (i.e., $y \\in \\mathbb{R}$) or discretely-valued (i.e., $y \\in \\{0,1,\\ldots,K\\}$). \n",
    "\n",
    "#### Model Selection\n",
    "\n",
    "The goal is to find some function $f$ in a fixed class of functions with the greatest prediction accuracy $f(\\vec{x}_j) \\approx y_j$ on the training set. To make this precise, we must choose a class of functions, a loss function and a regularization.\n",
    "\n",
    "- **Function Class:** Let $\\mathcal{F}$ be some collection of prediction functions. The collection should be finite-dimensional in the sense that it is parameterized by finitely many weights $\\vec{w} = (w_1,\\ldots,w_m)$. That is, any such vector of weights determines exactly one function in $\\mathcal{F}$ (and vice-versa).\n",
    "\n",
    "- **Loss Function:** A function $L(\\vec{w})$ which is minimized by the weight vector which produces the prediction function with the smallest prediction error. The form of the loss function depends on the data $X$ and $Y$. Generally $L$ is of the form $L = \\sum_j \\ell_j$, where $\\ell_j$ measures the error in predicting label $y_j$ by $f(\\vec{x}_j)$.\n",
    "\n",
    "- **Regularization:** A function $R(\\vec{w})$ which penalizes the complexity of the function determined by the weights $\\vec{w}$. \n",
    "\n",
    "#### Training\n",
    "\n",
    "We find our optimal function $f$ in the chosen class $\\mathcal{F}$ by finding the minimum value of $L(\\vec{w}) + R(\\vec{w})$. The weights realizing the minimum determine our classification function. The optimization procedure is generally carried out by some variant of gradient descent.\n",
    "\n",
    "#### Testing\n",
    "\n",
    "There is usually another dataset $X',Y'$ set aside on which we can test the accuracy of the function we determined.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Support Vector Machines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Motivation: Logistic Regression Revisited\n",
    "\n",
    "For simplicity, consider data $X = \\{\\vec{x}_1,\\ldots,\\vec{x}_n\\}$, $\\vec{x}_j \\in \\mathbb{R}^d$, and $Y = \\{y_1,\\ldots,y_n\\}$ with two labels ($y_j \\in \\{0,1\\}$). Classification by logistic regression really just looks for the best hyperplane $\\beta_1 x^1 + \\beta_2 x^2 + \\cdots + \\beta_n x^d + b = 0$ for separating the data; \"best\" means that the weight vector $\\vec{w} = (b,\\beta_1,\\ldots,\\beta_d)$ minimizes the cross-entropy loss function.\n",
    "\n",
    "Consider the following example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_blobs\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "X, y = make_blobs(centers=2, n_samples = 1000, center_box = [-4,4], random_state=1)\n",
    "\n",
    "fig = plt.figure(figsize=(8,6))\n",
    "plt.scatter(X[:,0], X[:,1], c=y)\n",
    "plt.title(\"Dataset\")\n",
    "plt.xlabel(\"First feature\")\n",
    "plt.ylabel(\"Second feature\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=1)\n",
    "\n",
    "model = LogisticRegression(solver='lbfgs', max_iter=10000)\n",
    "model.fit(X_train, y_train);\n",
    "model.score(X_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "xx, yy = np.mgrid[-10:10:.01, -10:10:.01]\n",
    "grid = np.c_[xx.ravel(), yy.ravel()]\n",
    "probs = model.predict_proba(grid)[:, 1].reshape(xx.shape)\n",
    "\n",
    "f, ax = plt.subplots(figsize=(8, 8))\n",
    "contour = ax.contourf(xx, yy, probs, 25, cmap=\"RdBu\",\n",
    "                      vmin=0, vmax=1)\n",
    "ax_c = f.colorbar(contour)\n",
    "ax_c.set_label(\"$P(y = 1)$\")\n",
    "ax_c.set_ticks([0, .25, .5, .75, 1])\n",
    "\n",
    "ax.scatter(X[:,0], X[:,1], c=y)\n",
    "\n",
    "ax.set(aspect=\"equal\",\n",
    "       xlim=(-10, 8), ylim=(-10, 8),\n",
    "       xlabel=\"First feature\", ylabel=\"Second feature\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The values of the probability function tell use that points on one side of the hyperplane get labeled $0$ and points on the other side get labeled $1$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Support Vector Machine\n",
    "\n",
    "Support Vector Machine is another approach to finding a separating hyperplane. Given a weight vector $\\vec{w} = (b,\\beta_1,\\ldots,\\beta_d)$, our prediction function now takes the simplistic form\n",
    "$$\n",
    "h(\\vec{x}) = \\left\\{\\begin{array}{cc}\n",
    "1 & b + \\beta_1 x^1 + \\cdots + \\beta_d x^d \\geq 0 \\\\\n",
    "0 & \\mbox{otherwise}\\end{array}\\right..\n",
    "$$\n",
    "I.e., the label of $\\vec{x}$ is determined by the side of the hyperplane that it lives on.\n",
    "\n",
    "For SVM, it is easier to write the loss function if we take our labels as $y_j \\in \\{-1,1\\}$. In this case, the loss function is called *hinge loss* and is given by the formula\n",
    "$$\n",
    "L(\\vec{w}) = \\sum_j \\max \\{0,1-y_j(b + \\beta_1 x^1 + \\cdots + \\beta_d x^d)\\}.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise\n",
    "\n",
    "Does this loss function make sense? Consider the function\n",
    "$$\n",
    "g(p) = \\max \\{0, 1-y \\cdot p\\},\n",
    "$$\n",
    "where $y \\in \\{-1,1\\}$. Think of $p$ as the \"prediction\"; if $p \\geq 0$ we predict the label $1$ and if $p < 0$ we predict the label $-1$. The claim is that correct predictions yield small values of this function and incorrect predictions yield large values. Convince yourself of this by either creating a plot or testing the output for some relevant values of $y$ and $p$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SVM typically includes an $\\ell_2$ regularization. It can be cast as an optimization problem of a smooth function on a constrained set. It can therefore be solved by some variant of gradient descent.\n",
    "\n",
    "The SVM algorithm extends to multiclass classification by a one-versus-all algorithm. *Basically*, we train a classifier for each class, then the class of a sample is determined by a vote.\n",
    "\n",
    "Let's test it on the toy data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "modelSVM = SVC(kernel=\"linear\",C=1)\n",
    "# C is a regularization parameter. The default is C=1, so this is not necessary to include. It is here for demonstration.\n",
    "modelSVM.fit(X_train, y_train)\n",
    "print(modelSVM.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can plot the *decision boundary* (i.e. the optimal separating hyperplane)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xx, yy = np.mgrid[-10:10:.01, -10:10:.01]\n",
    "grid = np.c_[xx.ravel(), yy.ravel()]\n",
    "probs = modelSVM.predict(grid).reshape(xx.shape)\n",
    "\n",
    "f, ax = plt.subplots(figsize=(8, 8))\n",
    "contour = ax.contourf(xx, yy, probs, 25, cmap=\"RdBu\",\n",
    "                      vmin=0, vmax=1)\n",
    "\n",
    "\n",
    "ax.scatter(X[:,0], X[:,1], c=y)\n",
    "\n",
    "ax.set(aspect=\"equal\",\n",
    "       xlim=(-10, 8), ylim=(-10, 8),\n",
    "       xlabel=\"First feature\", ylabel=\"Second feature\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise\n",
    "\n",
    "Fit a SVM model to the MNIST data, using the same test/train split as above. How do the classification results compare to logistic regression? How do the classification scores depend on the regularization parameter?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In general Logistic regression and SVM give different results. Generally SVM can be more successful for high-dimensional data and if the data has outliers. It is usually worthwhile to try both approaches."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## $k$-Nearest Neighbors\n",
    "\n",
    "Another extremely simple supervised classification technique is called *$k$-Nearest Neighbors*. To determine the class of a vector $\\vec{x}$, we look at the $k$ nearest points in the training data with respect to, say, Euclidean distance. The most common label amongst those is neighbors is then assigned to $\\vec{x}$.\n",
    "\n",
    "Let's try it on MNIST."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier \n",
    "knn = KNeighborsClassifier(n_neighbors=5)\n",
    "knn.fit(MNIST_train, MNISTlabels_train)\n",
    "print(knn.score(MNIST_test, MNISTlabels_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remarks:\n",
    "- This simple method does even better than logistic regression or SVM! In general, comparative performance of the models will depend on the application (i.e., kNN does worse than LR on some data).\n",
    "\n",
    "- The fitting essentially takes no time, because all\n",
    "`fit` does is store all the training data.  This results in &ldquo;large&rdquo;\n",
    "models that require quite a bit memory.  \n",
    "\n",
    "- For large datasets, `predict` is slow\n",
    "if there are many points, because it is costly to search for nearby\n",
    "neighbors.\n",
    "\n",
    "- $k$-NN can also run into problems for high-dimensional data, due to the \"Curse of Dimensionality\" https://en.wikipedia.org/wiki/Curse_of_dimensionality"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another interesting thing about $k$-NN classification is that it doesn't actually require knowledge of the **vectors**, just the **distances** between them. This means that $k$-NN is well-defined in general metric spaces which may not be vector spaces. \n",
    "\n",
    "A *metric* is just a way to measure distance between pairs of points in some set (the *metric space*). More precisely, if the set is $X$, then the metric is a function $d:X \\times X \\rightarrow \\mathbb{R}$ satisfying certain 'distance-like' axioms (see https://en.wikipedia.org/wiki/Metric_space).\n",
    "\n",
    "Some alternative metrics on $\\mathbb{R}^d$ are built in to `scikit-learn`. A list of them is available here: https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.DistanceMetric.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise\n",
    "\n",
    "Experiment with $k$-NN classification of MNIST by changing the parameters. In particular:\n",
    "\n",
    "1) What happens if you change the number of neighbors? Make a plot of the classification rate over various values of $k$. \n",
    "\n",
    "2) Try some other metrics on $\\mathbb{R}^{64}$. Which metric does the *worst* job?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Limitations of Logistic Regression and SVM\n",
    "\n",
    "It is easy to cook up data sets for which methods based on hyperplane decision boundaries are not suitable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_circles\n",
    "\n",
    "X, y = make_circles(n_samples=1000, factor=0.5, noise=.1, random_state=1)\n",
    "\n",
    "fig = plt.figure(figsize=(8,6))\n",
    "plt.scatter(X[:,0], X[:,1], c=y)\n",
    "plt.xlim([-1.5, 1.5])\n",
    "plt.ylim([-1.5, 1.5])\n",
    "plt.title(\"Dataset\")\n",
    "plt.xlabel(\"First feature\")\n",
    "plt.ylabel(\"Second feature\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try to fit a logistic regression model to this data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LogisticRegression(solver='lbfgs', max_iter=10000)\n",
    "model.fit(X_train, y_train);\n",
    "model.score(X_test,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not a very good classification score! This should intuitively make sense, since we are trying to split the data by a hyperplane. We can also see what's going wrong using the following plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "xx, yy = np.mgrid[-2:2:.01, -2:2:.01]\n",
    "grid = np.c_[xx.ravel(), yy.ravel()]\n",
    "probs = model.predict_proba(grid)[:, 1].reshape(xx.shape)\n",
    "\n",
    "f, ax = plt.subplots(figsize=(8, 8))\n",
    "contour = ax.contourf(xx, yy, probs, 25, cmap=\"RdBu\",\n",
    "                      vmin=0, vmax=1)\n",
    "ax_c = f.colorbar(contour)\n",
    "ax_c.set_label(\"$P(y = 1)$\")\n",
    "ax_c.set_ticks([0, .25, .5, .75, 1])\n",
    "\n",
    "ax.scatter(X[:,0], X[:,1], c=y)\n",
    "\n",
    "ax.set(aspect=\"equal\",\n",
    "       xlim=(-2, 2), ylim=(-2, 2),\n",
    "       xlabel=\"First feature\", ylabel=\"Second feature\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We would expect that SVM also doesn't work on this data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelSVM = SVC(kernel=\"linear\", C=0.025)\n",
    "modelSVM.fit(X_train, y_train)\n",
    "print(modelSVM.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "xx, yy = np.mgrid[-2:2:.01, -2:2:.01]\n",
    "grid = np.c_[xx.ravel(), yy.ravel()]\n",
    "probs = modelSVM.predict(grid).reshape(xx.shape)\n",
    "\n",
    "f, ax = plt.subplots(figsize=(8, 8))\n",
    "contour = ax.contourf(xx, yy, probs, 25, cmap=\"RdBu\",\n",
    "                      vmin=0, vmax=1)\n",
    "\n",
    "ax.scatter(X[:,0], X[:,1], c=y)\n",
    "\n",
    "ax.set(aspect=\"equal\",\n",
    "       xlim=(-2, 2), ylim=(-2, 2),\n",
    "       xlabel=\"First feature\", ylabel=\"Second feature\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise\n",
    "\n",
    "There are variants of Logistic Regression/SVM which will classify this data well. Here are a couple of things to try:\n",
    "\n",
    "1) The `SVC` model we used above for SVM has several options that can be changed. Try digging into the documentation a little bit and try playing with the options to get better classification https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html. Hint: `kernel`\n",
    "\n",
    "2) We did polynomial regression by using `PolynomialFeatures` to cast it as a linear regression problem. Try to do the same thing for logistic regression.\n",
    "\n",
    "3) There are many more supervised learning algorithms that we won't have time to discuss in detail. *Neural networks* are another popular choice. These fall into the general supervised learning framework discussed above, where the prediction functions are complicated functions whose structure mimics the neuron structure of a brain. For example, you can try the `MLPClassifier` documented here: https://scikit-learn.org/stable/modules/generated/sklearn.neural_network.MLPClassifier.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Homework\n",
    "\n",
    "The `fashion-mnist` is another interesting dataset for image classification. It contains preprocessed pictures of clothing items from 10 classes together with labels. The code below reads in and displays part of the `fashion-mnist` dataset. Explore this dataset and find which classification algorithm gives the best performance on it.\n",
    "\n",
    "**Note:** You will have to figure out how to get the data into an appropriate form to train the models. \n",
    "\n",
    "**Note:** This dataset is a lot bigger than what we have been using. The training steps may take a while!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import math\n",
    "# Load the data.\n",
    "data = pd.read_csv('fashion-mnist-demo.csv')\n",
    "\n",
    "# Create the mapping between numeric category and category name.\n",
    "label_map = {\n",
    "    0: 'T-shirt/top',\n",
    "    1: 'Trouser',\n",
    "    2: 'Pullover',\n",
    "    3: 'Dress',\n",
    "    4: 'Coat',\n",
    "    5: 'Sandal',\n",
    "    6: 'Shirt',\n",
    "    7: 'Sneaker',\n",
    "    8: 'Bag',\n",
    "    9: 'Ankle boot',\n",
    "}\n",
    "\n",
    "# Print the data table.\n",
    "data.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose a number of images to display. Should be a perfect square to make the display look good.\n",
    "numbers_to_display = 25\n",
    "\n",
    "# Calculate the number of cells that will hold all the images.\n",
    "grid_size = int(np.sqrt(numbers_to_display))\n",
    "\n",
    "plt.figure(figsize=(10, 10))\n",
    "\n",
    "# Go through the first images in a training set and plot them.\n",
    "for plot_index in range(numbers_to_display):\n",
    "    # Extrace image data.\n",
    "    digit = data[plot_index:plot_index + 1].values\n",
    "    digit_label = digit[0][0]\n",
    "    digit_pixels = digit[0][1:]\n",
    "\n",
    "    # Calculate image size (remember that each picture has square proportions).\n",
    "    image_size = int(np.sqrt(digit_pixels.shape[0]))\n",
    "    \n",
    "    # Convert image vector into the matrix of pixels.\n",
    "    frame = digit_pixels.reshape((image_size, image_size))\n",
    "    \n",
    "    # Plot the image matrix.\n",
    "    plt.subplot(grid_size, grid_size, plot_index + 1)\n",
    "    plt.imshow(frame, cmap='Greys')\n",
    "    plt.title(label_map[digit_label])\n",
    "    plt.tick_params(axis='both', which='both', bottom=False, left=False, labelbottom=False, labelleft=False)\n",
    "\n",
    "# Plot all subplots.\n",
    "plt.subplots_adjust(hspace=0.5, wspace=0.5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Homework\n",
    "\n",
    "Load the 'Olivetti faces' dataset https://scikit-learn.org/stable/modules/generated/sklearn.datasets.fetch_olivetti_faces.html#sklearn.datasets.fetch_olivetti_faces. You can do this using the commands\n",
    "\n",
    "`from sklearn.datasets import fetch_olivetti_faces`\n",
    "\n",
    "`data = fetch_olivetti_faces()`\n",
    "\n",
    "** Warning: ** This is a larger file and will take a minute to download. \n",
    "\n",
    "Train and test models for facial recognition based on this dataset. Try both logistic regression and $k$-NN. Which approach works better? Which pairs of faces confuse your models? For logistic regression, can you visualize what the coefficients are telling you?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
