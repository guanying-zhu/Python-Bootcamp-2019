{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# More Types of Expressions\n",
    "\n",
    "In this notebook we will spend some time manipulating and analyzing text documents. Before we do that, let's introduce a few more types of expressions that might be useful."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dictionaries\n",
    "\n",
    "Python dictionaries are similar to lists. List entries are indexed by their place in the list, but dictionary entries are indexed by keys. That is, Python dictionaries\n",
    "associate *values* to *keys*. Let's start with an example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = {'cat': 17, 'dog': -15, 'turtle': 0}\n",
    "# The keys are 'cat', 'dog', 'turtle'\n",
    "# Their respective values are 17, -15, 0\n",
    "d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can look-up values associated to a given key.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d['dog']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What happens if you try to access a missing key?\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = {'cat': 17, 'dog': -15, 'turtle': 0}\n",
    "print(d['purple'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Contrast that with the result of using `get`.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = {'cat': 17, 'dog': -15, 'turtle': 0}\n",
    "print(d.get('cat'))\n",
    "print(d.get('purple'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Values can be reassigned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = {'cat': 17, 'dog': -15, 'turtle': 0, 'fish': 100}\n",
    "print(d['cat'])\n",
    "d['cat'] = 1000\n",
    "print(d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Keys, along with associated values, can be removed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del(d['fish'])\n",
    "d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "They can also be added."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d['ferret']=900\n",
    "d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating Dictionaries\n",
    "\n",
    "There are several methods for creating dictionaries. Let's look at a few of them by example.\n",
    "\n",
    "In this first example, notice that the keys and the values are strings. Keys/values can be of virtually any Python expression type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "food_colors = {} # Start with an 'empty dictionary'\n",
    "food_colors['apple'] = 'red' # Add keys and their values\n",
    "food_colors['carrot'] = 'orange'\n",
    "food_colors['lemon'] = 'yellow'\n",
    "\n",
    "food_colors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can iterate over keys in a dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key in food_colors:\n",
    "    print('The color of '+key+' is '+food_colors[key])    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is another example of creating a dictionary. Note that the types of keys/values can be mixed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "L = [['cat','apple'],[100,'dog'],['a',9]] # Create a list of pairs\n",
    "\n",
    "d = dict(L)\n",
    "# The dict function uses the first entry in each pair of L as a key and the second as \n",
    "# the associated value.\n",
    "d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just like list comphrehension, there is dictionary comprehension."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fruits = ['apple', 'mango', 'banana','cherry']\n",
    "\n",
    "word_lengths = {f:len(f) for f in fruits}\n",
    "\n",
    "word_lengths"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise\n",
    "\n",
    "Experiment with dictionary creation to determine what types of objects are allowed as keys in a list and what types of objects are allowed as values. \n",
    "\n",
    "In particular:\n",
    "\n",
    "1) Find one expression type which can't be used as a key.\n",
    "\n",
    "2) Determine whether that object type can be used as a value.\n",
    "\n",
    "3) Can keys be repeated? Can values be repeated?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise\n",
    "\n",
    "Use a dictionary to make a \"menu\" containing five foods and their prices. Use your dictionary (and probably something like a `for` loop) to print out a readable menu."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Some Other Dictionary Methods\n",
    "\n",
    "Here are a few other things you can do with dictionaries which might prove useful (by no means an exhaustive list!). Let's define a list to test things on, then determine the purpose of each operation below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "animal_noises = {'cat':'meow','dog':'woof','cow':'moo'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(animal_noises.keys())\n",
    "# What happens if we don't plug this into the list function?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(animal_noises.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(animal_noises.items())\n",
    "\n",
    "# The output is a list. What is the type of each element in the list?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We met `get` earlier, but there is more to `get`.  For instance, there\n",
    "is a facility for including default values instead of `None`.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "default = 'No such animal'\n",
    "print(animal_noises.get('cat',default))\n",
    "print(animal_noises.get('purple',default))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is also `pop`.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(animal_noises)\n",
    "print(animal_noises.pop('cat'))\n",
    "print(animal_noises)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What if we want to combine two dictionaries?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "animal_noises2 = {'bird':'squawk','fly':'buzz'} # Define another dictionary for testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "animal_noises + animal_noises2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(animal_noises,animal_noises2)\n",
    "animal_noises.update(animal_noises2)\n",
    "print(animal_noises)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What happens if we combine dictionaries with some repeated keys?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experiment to figure it out!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sets\n",
    "\n",
    "Sets are similar to lists, except they are unordered and can't contain duplicate entries (just like mathematical sets!). Standard set operations like intersection, union, etc. can be performed on set objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "animals = ['cat','dog','cat','cow','cow']\n",
    "animal_set = set(animals)\n",
    "\n",
    "print(animals)\n",
    "print(animal_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "animal_set2 = set(['ferret','goat','cow','dog'])\n",
    "\n",
    "S = animal_set.intersection(animal_set2)\n",
    "\n",
    "print(animal_set2)\n",
    "print(S)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise\n",
    "\n",
    "Try to guess the syntax to take the union of `animal_set` and `animal_set2`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis of Movie Synopses\n",
    "\n",
    "Now we will apply our Python skills to analyze a dataset of movie synopses from Wikipedia. We will use data that I borrowed from https://github.com/brandomr/document_cluster. It includes a list of 100 movie titles, together with their synopses from Wikipedia. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing and Cleaning the Data\n",
    "\n",
    "The first step is to load the data. As usual, we have to clean the data (although these datasets were chosen because they are already pretty clean!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Upload the raw data for the movie titles\n",
    "f = open('data/title_list.txt')\n",
    "titles_raw_data = f.read()  \n",
    "f.close()\n",
    "print(titles_raw_data)\n",
    "len(titles_raw_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The titles data is one long string. It looks like we should split over new lines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titles = titles_raw_data.split('\\n')\n",
    "len(titles)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are only 100 movie titles. The last line is blank, hence the 101 length. Let's slice off the last entry."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titles = titles[:-1]\n",
    "len(titles)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally we load the synopses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open('data/synopses_list_wiki.txt')\n",
    "synopses_raw_data = f.read()  \n",
    "f.close()\n",
    "print(len(synopses_raw_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once again, the data is loaded as a single long string. Let's take a look at it to see how we should split it up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "synopses_raw_data[:100000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks like the different synopses are separated by 'BREAKS HERE'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "synopses_list = synopses_raw_data.split('BREAKS HERE')\n",
    "len(synopses_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once again there is an extra entry."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "synopses_list = synopses_list[:-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at the synopsis for the first movie (The Godfather)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "synopses_list[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is an annoying preamble which must come from Wikipedia, which should really be cut off. I'll do something hacky: copy and paste the offensive text, get its length, then just slice off the start of each synopsis by that length (there is probably a more elegant solution here...)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "synopses = [x[len(' Plot  [edit]  [  [  edit  edit  ]  ]  \\n  '):] for x in synopses_list]\n",
    "#synopses = list(map(lambda x:x[42:],synopses))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Check that the solution worked by looking at a few examples\n",
    "synopses[5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenizing and Removing Stop Words\n",
    "\n",
    "Now our goal is to compare the movie synopses. Of course, any block of natural language text will contain many instances of common words like 'the', 'an', etc. As the next preprocessing step, we would like to turn each synopsis into a list of words with these common words (called \"stop words\") removed.\n",
    "\n",
    "We'll use a package called `nltk` (Natural Language Tool Kit) The `nltk` has a list of stop words built in, which may take a second to download."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk # Import nltk\n",
    "# Import and download stop words\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "# Import the 'word_tokenize' function\n",
    "from nltk.tokenize import word_tokenize\n",
    "# The following is used in the tokenize function\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Store the stopwords in a list and take a look at them\n",
    "stop_words = stopwords.words('english')\n",
    "stop_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the plan is to split each synopsis into a list of words, then remove all stop words. We will use the `word_tokenize` function from `nltk`.\n",
    "\n",
    "We could also just use `.split(' ')`. The benefit here is that `word_tokenize` will also split off punctuation into its own entry, which will make our lives a bit easier. There will still be some work to do to clean up the tokenized word list, so let's start by working through an example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example = synopses[0]\n",
    "\n",
    "word_tokens = word_tokenize(example)\n",
    "\n",
    "word_tokens[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise\n",
    "\n",
    "We see that `','` appears as a word in the list. We would like to remove punctuation from this list. Define a new list called `word_tokens2` where all entries of the above list containing only punctuation are removed. I've helped you out by creating a list of stuff that we want to remove."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the list of extra stuff to remove\n",
    "punctuation = ['.',',',';',':',\"'s\",' ',\"' '\",\"'\",\"``\",\"''\",'(',')',\"'re\",'!','?','_','...','$','[',']']\n",
    "\n",
    "# Define the word_tokens2 list below.\n",
    "\n",
    "# Once it is defined, take a look at the result to make sure it worked.\n",
    "word_tokens2[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the `stopwords` set only contains words in all lower case. We can transfer each word in `word_tokens` into lower case using the `.lower()` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_tokens3 = list(map(lambda x:x.lower(),word_tokens2))\n",
    "\n",
    "word_tokens3[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Almost there! Now we can remove the stop words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#word_tokens4 = list(filter(lambda x: x not in stop_words,word_tokens3))\n",
    "word_tokens4 = [x for x in word_tokens3 if x not in stop_words]\n",
    "\n",
    "word_tokens4[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks like we have obtained the desired result. Now we can define a function to automate the above procedure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def synopsis_filter(synopsis):\n",
    "    # Get the initial list of word tokens\n",
    "    word_tokens = word_tokenize(synopsis)\n",
    "    # Remove punctuation. No need to keep making new variables, we can just update the old one.\n",
    "    word_tokens = [x for x in word_tokens if x not in punctuation]\n",
    "    # Transform words to lower case\n",
    "    word_tokens = [x.lower() for x in word_tokens]\n",
    "    # Remove stop words\n",
    "    word_tokens = [x for x in word_tokens if x not in stop_words]\n",
    "    return word_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test on a couple of examples to make sure it works\n",
    "synopsis_filter(synopses[3])[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Applying `synopsis_filter` to our list of synopses produces a nice data set with only the interesting words from each synopsis.\n",
    "\n",
    "Out of curiosity, I did this with both `map` and list comprehension and compared computing time. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "start = time.time()\n",
    "filtered_synopses = list(map(synopsis_filter,synopses))\n",
    "end = time.time()\n",
    "\n",
    "print(end-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "filtered_synopses_LC = [synopsis_filter(x) for x in synopses]\n",
    "end = time.time()\n",
    "\n",
    "print(end-start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trying the above a couple of times, it looks like `map` does provide some small speed advantage. If you are doing a more intense computation, it could be that the speed gain is not trivial!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at a couple of examples to make sure our data looks good\n",
    "filtered_synopses[3][:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Common Words Between Synopses\n",
    "\n",
    "One way that we could understand similarity between synopses would be to count the number of common words between them. This is easily accomplished by using set objects and taking intersections.\n",
    "\n",
    "To start, let's compare all synopses to the synopsis for The Godfather."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Define the Godfather word set\n",
    "Godfather_word_set = set(filtered_synopses[0])\n",
    "\n",
    "# For every other synopsis, find its word set, intersect it with the word set for The Godfather\n",
    "# then compute the magnitude of the result.\n",
    "same_words_count = [len(set(synopsis).intersection(Godfather_word_set)) for synopsis in filtered_synopses]\n",
    "\n",
    "# We can now plot these similar word counts.\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(same_words_count[1:])\n",
    "# Plot starting at index 1.\n",
    "# Godfather has all words in common with itself, so if we plot from index 0 the graph is not very informative."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that there are couple of outliers here. Let's check the titles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can find the locations of these outliers using the argmax and argmin functions from numpy\n",
    "import numpy as np\n",
    "most_similar = np.argmax(same_words_count[1:])\n",
    "least_similar = np.argmin(same_words_count)\n",
    "\n",
    "print('The most similar movie to The Godfather is '+titles[most_similar+1])\n",
    "# We need to reshift the 'most similar' result because we indexed it differently in our calculation.\n",
    "print('The least similar movie to The Godfather is '+titles[least_similar])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we've done this for a single movie, we can compare all pairs of movies. We just have to run the above computation for each movie."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movie_similarities = []\n",
    "for j in range(100):\n",
    "    word_set = set(filtered_synopses[j])\n",
    "    same_words_count = [len(set(synopsis).intersection(word_set)) for synopsis in filtered_synopses]\n",
    "    movie_similarities.append(same_words_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check that the lists make sense\n",
    "movie_similarities[2][:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check that this agrees with our previous work by plotting\n",
    "plt.plot(movie_similarities[0][1:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then define a list where the $k$th index gives the index of the movie which is least similar to $k$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "least_similar_movies = [np.argmin(movie_similarities[k]) for k in range(100)]\n",
    "least_similar_movies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hmmm... Looks like Yankee Doodle Dandy has a synopsis which is least similar to lots of movies. We might want to run that again, but exclude Yankee Doodle Dandy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "least_similar_movies = [np.argmin(movie_similarities[k][:99]) for k in range(100)]\n",
    "least_similar_movies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now create a dictionary with a key for each film and the values given by least similar movies. This will be a lot easier to read."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "least_similar_dict = {}\n",
    "\n",
    "for j in range(100):\n",
    "    least_similar_dict[titles[j]] = titles[least_similar_movies[j]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "least_similar_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we used a dictionary, we can look up least similar movies by name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "least_similar_dict['Rocky']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise\n",
    "\n",
    "Create a list `most_similar_movies` such that the $k$th entry gives the index of the movie synopsis which is *most* similar to the $k$th movie's. \n",
    "\n",
    "Of course, each synopsis is most similar to itself. This means that the $k$th entry should be the movie synopsis which is most similar to the $k$th movie's *excluding the $k$th movie itself*.\n",
    "\n",
    "Also define a dictionary called `most_similar_dict` in analogy with `least_similar_dict`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Homework\n",
    "\n",
    "The most similar movies list seems to be dominated by The Deer Hunter, All Quiet on the Western Front and Nashville. It might be interesting to remove these movies as options and rerun the 'most similar' computation. Give this a try. You may have to be creative, and you should definitely check that you are getting indexing correct!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Similarity by Word Counts\n",
    "\n",
    "Counting common words gives some interesting results, but there are many other ways we could try to measure similarity between texts.\n",
    "\n",
    "Let's try another method to measure similarity. We will first create a list containing all unique words which appear in all 100 synopses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The first computation creates one big list out of synopses (a list of lists)\n",
    "flat_word_list = [word for synopsis in filtered_synopses for word in synopsis]\n",
    "\n",
    "# Note the nexted 'for' loops in the above. Order matters here. \n",
    "# The code below gives an error\n",
    "#flat_word_list = [word for word in synopsis for synopsis in filtered_synopses]\n",
    "\n",
    "# Now we want to remove repeat words. \n",
    "word_list_no_repeats = list(set(flat_word_list)) # A set object doesn't have repeated entries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Out of curiosity, let's look at how much was removed by trimming our list to unique words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(flat_word_list), len(word_list_no_repeats)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can count the number of occurences of each word in each of the synopses to get a word vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_vectors = []\n",
    "\n",
    "for j in range(len(filtered_synopses)):\n",
    "    vec = []\n",
    "    synopsis = filtered_synopses[j] \n",
    "    for k in range(len(word_list_no_repeats)):\n",
    "        vec.append(synopsis.count(word_list_no_repeats[k]))\n",
    "        # Using the function .count(key) which counts the number of occurences of key in the list \n",
    "    word_vectors.append(vec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each word vector is a vector in $\\mathbb{R}^{10902}$. Since the synopses vary quite a lot in length, it might be useful to turn our word vectors into unit vectors. The result gives a collection of 100 points on the $10901$-dimensional unit sphere!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "normalized_word_vectors = [x/np.linalg.norm(x) for x in word_vectors]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's define a function to measure the angle between unit vectors. If we think of each unit vector as a point on the unit sphere, then the angle between the vectors gives a measure of distance between the points (i.e., the angle function is a *metric* on the sphere, called *geodesic distance*)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def angle(u,v):\n",
    "    uu = u/np.linalg.norm(u)\n",
    "    vv = v/np.linalg.norm(v)\n",
    "    if np.dot(uu,vv) > 0.999999: # Guard against numerical errors with the arccos function\n",
    "        return 0\n",
    "    else:\n",
    "        return np.arccos(np.dot(uu,vv))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test; the output should be pi/2 ~ 1.57\n",
    "u = np.array([1,0,0])\n",
    "v = np.array([0,1,0])\n",
    "angle(u,v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test; the output should be 0 \n",
    "angle(normalized_word_vectors[0],normalized_word_vectors[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at the angle between The Godfather and every other movie."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "angles_to_godfather = [angle(normalized_word_vectors[0],x) for x in normalized_word_vectors]\n",
    "\n",
    "plt.plot(angles_to_godfather[1:])\n",
    "# Once again, start plotting at 1 to get a better looking plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that almost everything is at around $\\pi/2$. This is actually not surprising if you are familiar with the peculiarities of high-dimensional geometry (see, e.g., https://en.wikipedia.org/wiki/Concentration_of_measure). \n",
    "\n",
    "The angles less than, say, $1.5$ really stand out here. Let's see which movies they are."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a list of indices for movies which are close to Godfather\n",
    "L = [idx for idx in range(len(angles_to_godfather)) if angles_to_godfather[idx] < 1.5]\n",
    "\n",
    "# Print the titles\n",
    "for j in range(len(L)):\n",
    "    print(titles[L[j]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise\n",
    "\n",
    "Create dictionaries listing most and least similar movies (as we did above), using the word frequency vectors. Do the results look any different than what we got above?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework\n",
    "\n",
    "There is another file in the data folder which lists all of the genre classifications for the movies. Try to use these methods to classify movie genres. Here is one possible scheme:\n",
    "\n",
    "For each movie, find the (say) 5 most similar movies via one of the methods above. Does the query movie share a genre classification with any of its 5 \"nearest neighbors\"? Out of the 100 movies in the list, how often do we get a successful classification? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework\n",
    "\n",
    "There are many other methods that can be used to compare text. A standard method, which is pretty close to our last approach, is to create vectors using *term frequency - inverse document frequency* scores. You can read about them here: https://en.wikipedia.org/wiki/Tf–idf. Try running through the experiments we did above using TF-IDF instead. Do you get any interesting results?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework\n",
    "\n",
    "Read up on Zipf's Law: https://en.wikipedia.org/wiki/Zipf%27s_law. Do the words in all of these Wikipedia articles follow Zipf's Law? To determine the answer, you will want to find the number of times that each word appears across all of the synopses; you might try Googling around to find an efficient way to perform a task like that (or you can always try to figure it out yourself!)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Big (Vague) Homework\n",
    "\n",
    "Start working your own coding project! After this week, you have seen most of the basics of Python---certainly enough to start working on something interesting. I highly recommend working on something that is related to your research, or working on analyzing some data set you personally find interesting. Having a running project is the best way to develop your programming skills!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
