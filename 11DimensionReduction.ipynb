{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dimension Reduction\n",
    "\n",
    "We now explore some techniques for visualizing high dimensional data. We begin with an application of Principal Component Analysis (PCA) to that end."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PCA as Dimension Reduction\n",
    "\n",
    "Let's return to our favorite dataset, MNIST."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_digits\n",
    "digits = load_digits()\n",
    "digits.data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recal that MNIST contains handwritten digits as $8 \\times 8$ images. We can think of these as vectors in $\\mathbb{R}^{8 \\times 8} \\approx \\mathbb{R}^{64}$.\n",
    "\n",
    "In the settings below, I smoothed the displays a bit to get smoother looking images. The real data still consists of the low-resolution images we've been using. See the documentation if you want to try some other options https://matplotlib.org/gallery/images_contours_and_fields/interpolation_methods.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig = plt.figure(figsize=(20,10))\n",
    "\n",
    "for j in range(10):\n",
    "    fig.add_subplot(1,10,j+1)\n",
    "    plt.imshow(digits.images[j], cmap='gray', interpolation = 'sinc')\n",
    "    # Using different display options than what we used previously"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can think of the MNIST dataset as a *point cloud* in $\\mathbb{R}^{64}$. This means it is impossible to visualize directly. On the other hand, we've seen that the data is structured enough that simple classifiation algorithms like logistic regression and SVM work extremely well. This leads us to believe that we may be able to get some sort of visualization of the data by exploiting its special structure.\n",
    "\n",
    "We begin by applying PCA to MNIST."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca = PCA()\n",
    "pca.fit(digits.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pca.components_.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we look at the explained variance ratios, we observe the following. Even though the data lives in $64$ dimensions, if we use the principal vector basis then any direction after the first 10 contributes less than 2\\% of the total variance in the data!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.plot(pca.explained_variance_ratio_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we only used the first couple of principal vectors as our coordinate axes, we might actually get a reasonable picture of the MNIST dataset!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pVec0 = pca.components_[0]\n",
    "pVec1 = pca.components_[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pVec0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Even though each principal vector lives in $\\mathbb{R}^{64}$, the first two of them still span a 2-dimensional plane. We can orthogonally project each point from the MNIST dataset onto this 2-dimensional plane. This is accomplished by taking dot products with each of the principal vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "projectedMNIST = []\n",
    "\n",
    "for j in range(1797):\n",
    "    projectedMNIST.append([np.dot(digits.data[j],pVec0),np.dot(digits.data[j],pVec1)])\n",
    "\n",
    "projectedMNIST = np.array(projectedMNIST)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can plot the projected point cloud on this two dimensional plane."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,10))\n",
    "\n",
    "plt.scatter(projectedMNIST[:, 0], projectedMNIST[:, 1],\n",
    "            c=digits.target, alpha=0.5,\n",
    "            cmap=plt.cm.get_cmap('nipy_spectral_r', 10))\n",
    "\n",
    "plt.xlabel('pVec0')\n",
    "plt.ylabel('pVec1')\n",
    "plt.axis('equal')\n",
    "plt.colorbar();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This gives a relatively faithful picture of the 64-dimensional MNIST dataset in only 2-dimensions. We can see the separation in the classes and this makes it more clear why logistic regression and SVM did such a good job of separating the data.\n",
    "\n",
    "The procedure of finding a low-dimensional representation of high-dimensional data is called *dimension reduction*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Of course this dimension reduction procedure is built into `scikit-learn`. The same effect is achieved by the following code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components = 2)  # project from 64 to 2 dimensions\n",
    "projected = pca.fit_transform(digits.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize = (10,10))\n",
    "\n",
    "plt.scatter(projected[:, 0], projected[:, 1],\n",
    "            c=digits.target, alpha=0.5,\n",
    "            cmap=plt.cm.get_cmap('nipy_spectral_r', 10))\n",
    "\n",
    "plt.xlabel('component 1')\n",
    "plt.ylabel('component 2')\n",
    "plt.axis('equal')\n",
    "plt.colorbar();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing Variability in the Digits\n",
    "\n",
    "We can try to visualize what the principal directions are capturing in MNIST. We begin by pulling out all of the digits labeled '0'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zeroList = digits.target==0\n",
    "zeros = digits.data[zeroList]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can compute the 'average zero' and visualize it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mu = []\n",
    "for j in range(64):\n",
    "    mu.append(np.mean(zeros[:,j]))\n",
    "\n",
    "plt.imshow(np.array(mu).reshape(8,8),cmap='gray', interpolation = 'sinc') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is pretty typical for this sort of image data: the 'average' in the label class is a very symmetric representation of the class.\n",
    "\n",
    "Next we perform PCA and get a feel for how variability is captured by the principal directions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pca = PCA()\n",
    "pca.fit(zeros)\n",
    "\n",
    "print(pca.singular_values_[:10])\n",
    "\n",
    "plt.plot(pca.explained_variance_ratio_);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we can take a look at the variability along the first principal direction as follows. We center the average zero, then move incrementally along the first principal direction. The increments should be proportional to the first singular value. This is accomplished by the following code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Center the mean, store first principal vector and singular value\n",
    "zeros_centered = zeros - np.array(mu)\n",
    "pVec1 = pca.components_[0]\n",
    "pVal1 = pca.singular_values_[0]\n",
    "\n",
    "# Create a list of offsets, proportional to the first singular value.\n",
    "offset_list = [-0.4*pVal1, -0.3*pVal1, -0.2*pVal1, -0.1*pVal1, 0*pVal1, 0.1*pVal1, 0.2*pVal1, 0.3*pVal1, 0.4*pVal1]\n",
    "\n",
    "# Create a figure displaying the result of moving the average along the principal direction.\n",
    "fig = plt.figure(figsize=(20,5))\n",
    "\n",
    "for j in range(len(offset_list)):\n",
    "    offsetMu = np.array(mu) + offset_list[j]*pVec1\n",
    "    fig.add_subplot(1,len(offset_list),j+1)\n",
    "    plt.imshow(offsetMu.reshape(8,8), cmap='gray', interpolation = 'sinc')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apparently, the main variability in the zeros is in their overall size. Let's write a general algorithm to visualize other directions of variability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Define the principal vector\n",
    "pvec_index = 1\n",
    "\n",
    "pVec = pca.components_[pvec_index]\n",
    "pVal = pca.singular_values_[pvec_index]\n",
    "\n",
    "# Create a list of offsets, proportional to the first singular value.\n",
    "offset_list = [-0.4*pVal, -0.3*pVal, -0.2*pVal, -0.1*pVal, 0*pVal, 0.1*pVal, 0.2*pVal, 0.3*pVal, 0.4*pVal]\n",
    "\n",
    "# Create a figure displaying the result of moving the average along the principal direction.\n",
    "fig = plt.figure(figsize=(20,5))\n",
    "\n",
    "for j in range(len(offset_list)):\n",
    "    offsetMu = np.array(mu) + offset_list[j]*pVec\n",
    "    fig.add_subplot(1,len(offset_list),j+1)\n",
    "    plt.imshow(offsetMu.reshape(8,8), cmap='gray', interpolation='sinc')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise\n",
    "\n",
    "Run similar experiments for the other digit classes. (You don't really need to write new code to do this; you can just make some small changes to what we've done above.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Digits\n",
    "\n",
    "We can generate \"random\" handwritten zeros by sampling from the normal distribution with center at the average zero and covariance given by the covariance of the point cloud of zeros. This is accomplished by the code below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zeroList = digits.target==0\n",
    "zeros = digits.data[zeroList]\n",
    "\n",
    "mu = []\n",
    "for j in range(64):\n",
    "    mu.append(np.mean(zeros[:,j]))\n",
    "\n",
    "zeros_centered = zeros - np.array(mu)\n",
    "\n",
    "cov = zeros_centered.T@zeros_centered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(20,10))\n",
    "\n",
    "for j in range(10):\n",
    "    zeroSample = np.random.multivariate_normal(mu, .01*cov, 1).T\n",
    "    RandomZero = zeroSample.reshape(8,8)\n",
    "    fig.add_subplot(1,10,j+1)\n",
    "    plt.imshow(RandomZero, cmap='gray', interpolation='sinc')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare this to randomly selecting actual digits from the list of zeros."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(20,10))\n",
    "\n",
    "for j in range(10):\n",
    "    zeroSample = zeros[np.random.randint(0,len(zeros))]\n",
    "    RandomZero = zeroSample.reshape(8,8)\n",
    "    fig.add_subplot(1,10,j+1)\n",
    "    plt.imshow(RandomZero, cmap='gray', interpolation='sinc')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise/Homework\n",
    "\n",
    "Here is another classification scheme: compute the mean digit in each digit class. Then use 'distance to the mean' as your classifier: given a digit vector $\\vec{x}$, label it by the digit whose mean it is closest to. How does the classification rate compare to our other methods?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## T-SNE \n",
    "\n",
    "T-SNE (T-distributed Stochastic Neighbor Embedding) is another recently developed dimension reduction algorithm. The problem formulation is more involved than PCA. A basic description is as follows:\n",
    "\n",
    "#### Goal\n",
    "\n",
    "Given a data set $\\vec{x}_1,\\ldots,\\vec{x}_N$ of vectors $\\vec{x}_j \\in \\mathbb{R}^d$, where $d > 3$, we seek a set $\\vec{y}_1,\\ldots,\\vec{y}_N$ of vectors in $\\mathbb{R}^k$ with $k=2$ or $3$, such that the $\\vec{y}_j$ faithfully reflect the clustering behavior of the $\\vec{x}_j$.\n",
    "\n",
    "#### Similarity Scores\n",
    "\n",
    "Similarity between data points $\\vec{x}_i$ and $\\vec{x}_j$ is measured by\n",
    "$$\n",
    "p_{ij} = \\frac{1}{2N} \\left( \\frac{\\exp (-\\|\\vec{x}_i - \\vec{x}_j\\|^2/\\sigma_i^2)}{\\sum_{k \\neq i} \\exp(-\\|\\vec{x}_i - \\vec{x}_k\\|/\\sigma_i^2)} +  \\frac{\\exp (-\\|\\vec{x}_i - \\vec{x}_j\\|^2/\\sigma_j^2)}{\\sum_{k \\neq j} \\exp(-\\|\\vec{x}_j - \\vec{x}_k\\|/\\sigma_j^2)} \\right),\n",
    "$$\n",
    "where the widths $\\sigma_i$ are chosen by an algorithm so that smaller $\\sigma_i$ occur in denser parts of the point cloud.\n",
    "\n",
    "Similarity scores between points $\\vec{y}_i$ and $\\vec{y}_j$ is measured by\n",
    "$$\n",
    "q_{ij} = \\frac{\\left(1+\\|\\vec{y}_i - \\vec{y}_j\\|^2\\right)^{-1}}{\\sum_{\\ell \\neq p} \\left(1+\\|\\vec{y}_\\ell - \\vec{y}_p\\|^2\\right)^{-1}}\n",
    "$$\n",
    "for $i \\neq j$ and we set $q_{ii} = 0$.\n",
    "\n",
    "#### Loss Function\n",
    "\n",
    "We then search for a set of vectors $Y = \\{\\vec{y}_1,\\ldots,\\vec{y}_N\\}$ which minimize the following loss function, called *Kullback-Liebler divergence*:\n",
    "$$\n",
    "L(Y) = \\sum_{i \\neq j} p_{ij} \\log \\frac{p_{ij}}{q_{ij}}.\n",
    "$$\n",
    "This is minimized via some variant of gradient descent.\n",
    "\n",
    "### Drawbacks of T-SNE\n",
    "\n",
    "- The loss function is nonconvex and has many local minima. Gradient descent may converge to a \"bad\" local minimum.\n",
    "- It is computationally expensive compared to PCA, especially for very high-dimensional data.\n",
    "- Compared to PCA, it is not as readily interpretable. We don't get useful information like principal vectors which can be used to perform other tasks.\n",
    "\n",
    "### Pros of T-SNE\n",
    "\n",
    "It can work *amazingly* well!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's time the computation\n",
    "import time \n",
    "\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "start = time.time()\n",
    "digits_projected = TSNE(n_components=2).fit_transform(digits.data)\n",
    "end = time.time()\n",
    "\n",
    "print(end-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (10,10))\n",
    "\n",
    "plt.scatter(digits_projected[:, 0], digits_projected[:, 1],\n",
    "            c=digits.target, alpha=0.5,\n",
    "            cmap=plt.cm.get_cmap('nipy_spectral_r', 10))\n",
    "\n",
    "plt.xlabel('component 1')\n",
    "plt.ylabel('component 2')\n",
    "plt.axis('equal')\n",
    "plt.colorbar();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Almost perfect clustering! If we run it again we will get a diferent answer (because the loss function doesn't have a unique minimum)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "digits_projected = TSNE(n_components=2).fit_transform(digits.data)\n",
    "end = time.time()\n",
    "\n",
    "print(end-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize = (10,10))\n",
    "\n",
    "plt.scatter(digits_projected[:, 0], digits_projected[:, 1],\n",
    "            c=digits.target, alpha=0.5,\n",
    "            cmap=plt.cm.get_cmap('nipy_spectral_r', 10))\n",
    "\n",
    "plt.xlabel('component 1')\n",
    "plt.ylabel('component 2')\n",
    "plt.axis('equal')\n",
    "plt.colorbar();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Speeding up the Computation\n",
    "\n",
    "If you use this on a larger dataset, you may want to try to speed up the computation (this dataset is relatively small and the computation is taking ~12 seconds). One trick is to use PCA to reduce the dimension first (more modestly than all the way down to 2!), then run T-SNE.\n",
    "\n",
    "This trick doesn't really have an effect on the MNIST data, but it might be useful on a higher dimensional dataset. The code for achieving it is shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "pca = PCA(n_components = 2)  # project from 64 to 30 dimensions via PCA\n",
    "PCA_projected = pca.fit_transform(digits.data)\n",
    "\n",
    "digits_projected = TSNE(n_components=2).fit_transform(PCA_projected)\n",
    "end = time.time()\n",
    "\n",
    "print(end-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (10,10))\n",
    "\n",
    "plt.scatter(digits_projected[:, 0], digits_projected[:, 1],\n",
    "            c=digits.target, alpha=0.5,\n",
    "            cmap=plt.cm.get_cmap('nipy_spectral_r', 10))\n",
    "\n",
    "plt.xlabel('component 1')\n",
    "plt.ylabel('component 2')\n",
    "plt.axis('equal')\n",
    "plt.colorbar();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise/Homework\n",
    "\n",
    "Create a 3D scatterplot showing the projection of the MNIST data onto the first 3 principal directions. How does it compare to the 2D projection?\n",
    "\n",
    "Hints:\n",
    "- Look here for an idea of how to create 3D scatterplots https://matplotlib.org/2.1.1/gallery/mplot3d/scatter3d.html\n",
    "- Use the command `%matplotlib notebook` to get a 3D plot which you can drag to rotate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise/Homework\n",
    "\n",
    "Apply similar dimension reduction techniques on the `fashion-mnist` and/or `olivetti_faces` datasets that we have studied previously. Are these datasets similarly well-separated?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
